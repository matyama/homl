{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pediatric-jonathan",
   "metadata": {},
   "source": [
    "# Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gross-basic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected.\n"
     ]
    }
   ],
   "source": [
    "# FIXME: meke autocompletion working again\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if not physical_devices:\n",
    "    print(\"No GPU was detected.\")\n",
    "else:\n",
    "    # https://stackoverflow.com/a/60699372\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-multimedia",
   "metadata": {},
   "source": [
    "## Char-RNN\n",
    "Let's build a RNN processing sequences of text and predicting single character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-curve",
   "metadata": {},
   "source": [
    "### Loading the Data and Preparing the Dataset\n",
    "Following example uses famous Shakespear's texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reduced-thought",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download the dataset\n",
    "filepath = keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    ")\n",
    "\n",
    "# Load raw dataset\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "    \n",
    "# Show a pice of the text\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "studied-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a character-based text tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "split-facing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a text to a sequence of character IDs\n",
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extreme-grill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a sequence of character IDs back to text\n",
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "native-cache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "\n",
    "# total number of characters\n",
    "dataset_size = tokenizer.document_count\n",
    "\n",
    "# Encode the whole dataset\n",
    "#  - TF tokenizer assigns the first character it encounters with ID=1, we shift it back to start from 0\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "\n",
    "# Build a training TF Dataset from the first 90% of the text\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "# Preprocessing parameters\n",
    "# - length of a training instance (sequence of text)\n",
    "# - size of a training micro-batch\n",
    "n_steps = 100\n",
    "batch_size = 32\n",
    "\n",
    "# target = input shifted 1 character ahead\n",
    "window_length = n_steps + 1\n",
    "\n",
    "# Create training instances (sequences of text) by sliding a window over the text\n",
    "#  - each time we shift it by single character (`shift=1`)\n",
    "#  - `drop_remainder=True` means that we don't want to include final shortened windows with length < window length \n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "# Because `window()` creates a nested Dataset (containing sub-datasets), we want to flatten and convert it to single dataset of tensors\n",
    "#  - the trick here is that we batch the windows to the same length they already have\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Now we can safely shuffle the dataset and not to break the text\n",
    "#  - note: shuffling ensures some degree of i.i.d. which is necessary for SGD to work well\n",
    "#  - we also create training micro-batches\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "\n",
    "# Split the instances to (inputs, target) where the target is the next character\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "# As the last step we must either encode or embed categorical features (characters)\n",
    "#  - here we use 1-hot encoding since there's fairly few distinct characters\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# Finally we prefetch the data for better training performance\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Show shapes of 1st batch tensors\n",
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-tablet",
   "metadata": {},
   "source": [
    "### Creating and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "usual-plastic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 15s 229ms/step - loss: 3.4057\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 9s 210ms/step - loss: 2.9618\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 9s 207ms/step - loss: 2.6435\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 10s 221ms/step - loss: 2.4503\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 10s 230ms/step - loss: 2.3631\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 11s 238ms/step - loss: 2.2816\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 9s 206ms/step - loss: 2.2097\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 9s 197ms/step - loss: 2.1400\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 9s 195ms/step - loss: 2.0724\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 9s 195ms/step - loss: 2.0116\n"
     ]
    }
   ],
   "source": [
    "# Build a simple Char-RNN model:\n",
    "# - there are two GRU recurrent layers with 128 units, both of which use a 20% dropout (`recurrent_dropout`)\n",
    "# - there's also a 20% input dropout (`dropout` parameter of the 1st layer)\n",
    "# - the output layer is a time-distributed dense layer with 39 units and softmax activation to predict each character's class probability\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Train and validate the model for 10 epochs\n",
    "# - Note: This would take forever to train on my PC, so let's use just few batches\n",
    "history = model.fit(dataset.take(40), epochs=10)\n",
    "# history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-scotland",
   "metadata": {},
   "source": [
    "### Using the Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fifteen-duplicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(texts):\n",
    "    \"\"\"Preprocess given text to conform to Char-RNN's input\"\"\"\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "# Make a new prediction using the model\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "\n",
    "# Show the prediction as text: 1st sentence, last char\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-protection",
   "metadata": {},
   "source": [
    "Next, let's generate not only single letter but whole new text. One approach is to repeatedly call the above. However, this often leads to repeating the same letter over and over again. Better approach is to select next letter randomly based on the learned class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "domestic-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t the beall the beall the cour and the belly well t\n"
     ]
    }
   ],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    \"\"\"\n",
    "    Generate new characters based on given text.\n",
    "     1. we pre-process and predict as before but return all character probablilities\n",
    "     2. then we compute the log of probabilities and scale it by the `temperature` parameter (the higher, the more in favour of higher prob. letters)\n",
    "     3. finally we select single character randomly given these log-probs. and convert the character ID back to text \n",
    "    \"\"\"\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    \"\"\"Extend given text with `n_chars` new letters\"\"\"\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Complete some text using different temperatures\n",
    "#  - Note: this example dosn't present the model very well since it's not been trained on the full dataset\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "solar-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucio' theng; muke,\n",
      "ay, you the beagu, or us you wa\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "swedish-treaty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ty no c't;\n",
      "mest,-haigeatfrai' at:,\n",
      "mearbsgr:\n",
      "ges't.\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-cologne",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "The premise of a *Stateful RNN* is simple: So far we've thrown all neurons' hidden states away after applying BPTT on a training batch. In other words, hidden states were re-initialized for each partial update and so the model had hard time to learn long term patterns. The idea of a *Stateful RNN* is to keep the hidden state from previous batch and not to initialize it over again.\n",
    "\n",
    "This has, however, a consequence for the pre-processing logic. If we assume the state is transferred over from previous batches, these batches of training instances cannot overlap - they must consecutively extend each one. In our text generating example, this means we can't use overlapping windows and shuffling anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "drawn-vehicle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 56s 170ms/step - loss: 2.9070\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 57s 181ms/step - loss: 2.2857\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 56s 180ms/step - loss: 2.0682\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 52s 167ms/step - loss: 2.0733\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 56s 179ms/step - loss: 2.2716\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 56s 180ms/step - loss: 2.5184\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 57s 181ms/step - loss: 1.9737\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 57s 183ms/step - loss: 2.0247\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 58s 184ms/step - loss: 2.0970\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 58s 185ms/step - loss: 1.8134\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 58s 184ms/step - loss: 1.7855\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 59s 187ms/step - loss: 2.0163\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 58s 185ms/step - loss: 1.9225\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 59s 187ms/step - loss: 1.9329\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 59s 190ms/step - loss: 1.8944\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 61s 194ms/step - loss: 1.8220\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 61s 193ms/step - loss: 1.7429\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 1.7147\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 63s 200ms/step - loss: 1.6869\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 63s 202ms/step - loss: 1.6751\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 64s 204ms/step - loss: 1.6870\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 64s 205ms/step - loss: 1.6598\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 64s 205ms/step - loss: 1.6531\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 65s 206ms/step - loss: 1.6458\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 60s 190ms/step - loss: 1.6403\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 58s 186ms/step - loss: 1.6340\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 59s 188ms/step - loss: 1.6285\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 59s 189ms/step - loss: 1.6247\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 59s 188ms/step - loss: 1.6188\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 60s 192ms/step - loss: 1.6147\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 63s 201ms/step - loss: 1.6102\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 60s 192ms/step - loss: 1.6078\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 59s 188ms/step - loss: 1.6030\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 60s 193ms/step - loss: 1.6000\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 1.5979\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 61s 196ms/step - loss: 1.5940\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 1.5914\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 61s 195ms/step - loss: 1.5885\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 60s 190ms/step - loss: 1.5860\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 62s 197ms/step - loss: 1.5845\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 62s 199ms/step - loss: 1.5815\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 62s 199ms/step - loss: 1.5785\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 62s 199ms/step - loss: 1.5771\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 68s 218ms/step - loss: 1.5751\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 75s 240ms/step - loss: 1.5731\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 75s 240ms/step - loss: 1.5700\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 72s 229ms/step - loss: 1.5694\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 61s 194ms/step - loss: 1.5677\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 63s 203ms/step - loss: 1.5670\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 64s 203ms/step - loss: 1.5636\n"
     ]
    }
   ],
   "source": [
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# (a) Updated pre-processing logic for Stateful Char-RNN\n",
    "# - In this version we apply single window at a time\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "# Contrary to before, we shift windows by full `n_steps` to create non-overlapping inputs\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# We skip shuffling altogether so that we don't break the preserved state and batch by 1\n",
    "#  - batching by 1 means that we apply just single window at a time and, again, preserve the state\n",
    "dataset = dataset.repeat().batch(1)\n",
    "\n",
    "# The rest of the logic is analogous\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# (b) Updated pre-processing logic for Stateful Char-RNN\n",
    "# - In this more complicated version we apply a micro-batch of windows as before\n",
    "batch_size = 32\n",
    "\n",
    "@tf.function\n",
    "def make_windowed_ds(encoded_part):\n",
    "    \"\"\"Creates a flat windowed TF Dataset of non-overlapping windows\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    return dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Contrary to before, we make a windowed Dataset in two steps:\n",
    "#  1. We split the dateset into equal length batches and make windowed Dataset from each batch\n",
    "#  2. Then we put put all these batches back together and stack the windows so that \n",
    "#     the n-th inputs sequence of a batch starts where the n-th sequence of the previous one ended\n",
    "datasets = map(make_windowed_ds, np.array_split(encoded[:train_size], batch_size))\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "\n",
    "# Final steps are the same:\n",
    "#  - Split each window to (inputs, target)\n",
    "#  - 1-hot encode the categorical input features\n",
    "#  - Prefetch the data for better performance\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Build a Stateful RNN model\n",
    "# The architecture is basically the same as before, notice two distinctions:\n",
    "#  - `stateful=True` on the recurrent layers to preserve hidden state\n",
    "#  - `batch_input_shape` set for the initial recurrent layer to let the model know the shape (batch size) for the hidden state\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(\n",
    "        128,\n",
    "        return_sequences=True,\n",
    "        stateful=True,\n",
    "        dropout=0.2,\n",
    "        recurrent_dropout=0.2,\n",
    "        batch_input_shape=[batch_size, None, max_id],\n",
    "    ),\n",
    "    keras.layers.GRU(\n",
    "        128, \n",
    "        return_sequences=True,\n",
    "        stateful=True,\n",
    "        dropout=0.2,\n",
    "        recurrent_dropout=0.2,\n",
    "    ),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\")),\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Train and validate the model\n",
    "#  - we use custom callback to reset model's state at the start of each epoch (instead of each batch)\n",
    "#  - we train the model for 50 epochs, also notice the updated `steps_per_epoch`\n",
    "\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Callback that resets model's state each epoch\"\"\"\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    dataset, \n",
    "    steps_per_epoch=train_size // batch_size // n_steps,\n",
    "    epochs=50,\n",
    "    callbacks=[ResetStatesCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-parker",
   "metadata": {},
   "source": [
    "To use the model with different batch sizes, we need to create a stateless copy. We can get rid of dropout since it is only used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "formed-layout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thought thy fearing?\n",
      "\n",
      "coriolanus:\n",
      "no, buchio, my li\n"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a steteless Char-RNN model\n",
    "# - This model is based on our steteful Char-RNN but used only for making predictions\n",
    "# - Notice: We don't need dropout since it's used only during training\n",
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\")),\n",
    "])\n",
    "\n",
    "# Build the stateless model\n",
    "#  - Firstly, we can loosen the fixed batch size restriction\n",
    "#  - Secondly, we copy learned weights from the stateful model (this works fine since dropout layers have no trainable params)\n",
    "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
    "stateless_model.set_weights(model.get_weights())\n",
    "\n",
    "# Replace our main model by this one\n",
    "#  - because `complete_text()` implicitly works with `model`\n",
    "model = stateless_model\n",
    "\n",
    "# Try to complete some text\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-console",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Let's take a step further from the character-level RNNs to word-level sentiment analysis. Typical dataset from this taks is the IMDb reviews dataset, so let's play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fourth-pledge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/matyama/.cache/pypoetry/virtualenvs/homl-lPHrmr2i-py3.8/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/home/matyama/.cache/pypoetry/virtualenvs/homl-lPHrmr2i-py3.8/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the IMDb reviews dataset\n",
    "(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()\n",
    "\n",
    "# Show a training instance\n",
    "#  - The dataset is already preprocessed, each instance is a sequence integers which represent an ID of a word\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thrown-brick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to reconstruct a word we can load the word to ID index\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# And then create an inverse mapping\n",
    "# - Note: We shift the ID by 3 to reserve first three IDs for special markers\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "\n",
    "# These special markers are for the:\n",
    "#  - padding symbol\n",
    "#  - start of sequence\n",
    "#  - unknown word\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "    \n",
    "# Show a sample of decoded words\n",
    "\" \".join(id_to_word[id_] for id_ in X_train[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-share",
   "metadata": {},
   "source": [
    "Now, let's create the same pre-processing logic and trainable dataset using TensorFlow's Datasets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pacific-injury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the IMDb reviews TF Dataset\n",
    "#  - Note: Using TF-only functions allows us to reuse the same pre-processing logic in every environment\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "\n",
    "# List the dataset content\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "experienced-outreach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and show training and test set sizes\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples\n",
    "\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amino-persian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peek the training dataset\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sustainable-output",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    \"\"\"\n",
    "    Pre-process an input batch:\n",
    "     1. Crops each instance to first 300 characters (speeds up training and sentiment can usually be deduced by the first few sentences)\n",
    "     2. Replaces '<br />' symbols by a space character\n",
    "     3. Replaces each non-letter and quote character by a space\n",
    "     4. Splits instances by space creating a ragged tensor\n",
    "     5. Returns a dense tensor (and original label) made by padding the splits with '<pad>'\n",
    "    \"\"\"\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
    "\n",
    "# Try the preprocessing logic on the first training batch\n",
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "regional-excitement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Do a word-count over the whole pre-processed training dataset (in one pass)\n",
    "vocabulary = Counter(\n",
    "    word.numpy()\n",
    "    for X_batch, _ in datasets[\"train\"].batch(batch_size).map(preprocess)\n",
    "    for review in X_batch\n",
    "    for word in review\n",
    ")\n",
    "\n",
    "# Show first 3 most common words in the training corpus\n",
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "brazilian-sunday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bibliographic-workshop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "12\n",
      "11\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Drop the least important words and keep just 10k most frequent ones\n",
    "vocab_size = 10_000\n",
    "truncated_vocabulary = [word for word, _ in vocabulary.most_common(vocab_size)]\n",
    "\n",
    "# Make a word index from the truncated vocabulary\n",
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "\n",
    "# Test the word index on an example sentence\n",
    "for word in b\"This movie was faaaaaantastic\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "considerable-essence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a static vocabulary table with 1k OOV buckets\n",
    "num_oov_buckets = 1000\n",
    "\n",
    "# Initialize the vocabulary from our truncated vocabulary and word index\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "\n",
    "# Build the lookup table\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "# Test the lookup table on the example sentence we used before\n",
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sensitive-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    \"\"\"Encode each word in an input batch using the static vocabulary table\"\"\"\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "# Preprocess and encode the whole training set\n",
    "train_set = (\n",
    "    datasets[\"train\"]\n",
    "    .repeat()\n",
    "    .batch(batch_size)\n",
    "    .map(preprocess)\n",
    "    .map(encode_words)\n",
    "    .prefetch(1)\n",
    ")\n",
    "\n",
    "# Display the 1st training batch\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "entertaining-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 83s 98ms/step - loss: 0.5957 - accuracy: 0.6606\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 82s 104ms/step - loss: 0.3701 - accuracy: 0.8398\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 84s 108ms/step - loss: 0.2081 - accuracy: 0.9237\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 88s 113ms/step - loss: 0.1412 - accuracy: 0.9512\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 86s 110ms/step - loss: 0.1072 - accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "# The embedding dimention hyperparameter\n",
    "embed_size = 128\n",
    "\n",
    "# Build a classification RNN with initial word embedding layer\n",
    "#  - This layer's matrix has shape [ID count = vocabulary size + OOV buckets, embedding dimension]\n",
    "#  - So the model's inputs are 2D tensors of shape [batch size, time steps], the embedding output is 3D tensor [batch size, time steps, embedding size]\n",
    "#  - `mask_zero=True` means that we ignore ID=0 - the most frequent word which in our case is `<pad>` (so the model doesn't have to learn to ignore it)\n",
    "#  - note: It would clearner to ensure that the padding word really has ID 0 than to count on the fact that it's the most frequent one.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, mask_zero=True, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train and validate the model for 5 epochs\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-football",
   "metadata": {},
   "source": [
    "### Manual Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "higher-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 80s 95ms/step - loss: 0.6093 - accuracy: 0.6406\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 90s 116ms/step - loss: 0.3711 - accuracy: 0.8425\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 133s 171ms/step - loss: 0.1953 - accuracy: 0.9286\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 121s 155ms/step - loss: 0.1205 - accuracy: 0.9582\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 120s 154ms/step - loss: 0.1056 - accuracy: 0.9631\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "\n",
    "# Define an input layer\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "\n",
    "# Create a mask that ignores inputs equal to 0\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "\n",
    "# Build the same model structure as before but with explicit masking of layer inputs\n",
    "#  - Note: In the previous example the output dense layer didn't receive the implicit mask because the time dimension was not the same, \n",
    "#          so the explicit masking is necessary if we want to propagate this information all the way to the loss function.\n",
    "#  - Note 2: The downside is that LSTMs and GRUs won't use optimized impl. for GPUs and so the training might be slower.\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "\n",
    "# Define model's outputs\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "# Compose and compile the model\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train and validate the model for 5 epochs\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-membrane",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "competitive-pontiac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 5s 5ms/step - loss: 0.5861 - accuracy: 0.6919\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 4s 5ms/step - loss: 0.5181 - accuracy: 0.7445\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 3s 4ms/step - loss: 0.5122 - accuracy: 0.7494\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 3s 4ms/step - loss: 0.5086 - accuracy: 0.7492\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 4s 5ms/step - loss: 0.5052 - accuracy: 0.7518\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build a model with pre-trained layers:\n",
    "#  - Main portion of this model reuses Google's model that pre-processes and embeds words from an input text to 50 dimensional vectors\n",
    "#  - Then we just add two dense layers for our classification task of sentiment analysis\n",
    "#  - Note: By default TF Hub downloads models to /tmp, one can override this by setting `TFHUB_CACHE_DIR` env. variable\n",
    "#  - Note 2: TF Hub layers are also by default non-trainable - if we want to tweak their weights we must unfreeze them\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "        dtype=tf.string,\n",
    "        input_shape=[],\n",
    "        output_shape=[50],\n",
    "    ),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Then we can just load the IMDb reviews dataset\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "\n",
    "# Take the training set and just batch it (and prefetch)\n",
    "#  - Note: The rest of the preprocessing logic is handled by the TF Hub portion of the model\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
    "\n",
    "# Finally we just train and validate the model on our IMDb dataset\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-tournament",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Network for Neural Machine Translation\n",
    "\n",
    "As the name suggests, in the *Encoder-Decoder* architecture we split a *sequence-to-sequence* RNN into two parts:\n",
    "1. Encoder - takes as inputs reversed sequences of words (or rather embeddings thereof; reversed so that the decoder reveives the first word first)\n",
    "1. Decoder - this part has actually two inputs, first the hidden states of the encoder and socond is either previous target word (during training; embedded) or the actual token that was output in the previous step (during inference; embedded)\n",
    "\n",
    "Additional notes to the architecture:\n",
    "* The outputs of the decoder are scores for each word in the vocabulary which are turned to probabilities using time-distributed *softmax*. Because we can easily get to very high-dimensional outputs, typically a *sampled softmax* is used for training and regular *softmax* for inference\n",
    "* In this task we cannot simply truncate input sequences to common length as before because we want to get complete translations. Also pedding to some large common lenght does not work. Instead, we can bucket the sentenced into sets of close-enough lenght and pad these to match the longes one in each set.\n",
    "* Finally, we should ignore part of the output after an `<EOS>` token - both from the output and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "perfect-replication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 13s 224ms/step - loss: 4.6052\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 7s 209ms/step - loss: 4.6024\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Set the RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Sutup vocabulary and embedding size hyperparameters\n",
    "vocab_size = 100\n",
    "embed_size = 10\n",
    "\n",
    "# Define Encoder and Decoder inputs\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "# Create embedding layers for the Encoder and Decoder parts\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "# Encoder is a 512 unit LSTM layer\n",
    "#  - we can ignore encoder ouputs but we return both the short-term and long-term states with `return_state=True`\n",
    "#  - the complete hidden state of the encoder is a pair of the short and long-term states\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "_, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "# Decoder is based on the `BasicDecoder` from TF Addons\n",
    "#  - Decoder cell is a 512 unit LSTM cell\n",
    "#  - Sampler is a component tells the Decoder what it should pretend the last step's output was:\n",
    "#    - in this case `TrainingSampler` takses the embedding of previous target token\n",
    "#    - other option is `ScheduledEmbedingTrainingSampler` which randomly chooses between target and actual outputs\n",
    "#  - Model's output is a dense layer with one unit per word in the vocabulary\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    cell=decoder_cell,\n",
    "    sampler=tfa.seq2seq.sampler.TrainingSampler(),\n",
    "    output_layer=output_layer,\n",
    ")\n",
    "\n",
    "# Construct the Decoder\n",
    "#  - Initial state is the complete encoder state\n",
    "#  - We can ignore final decoder state and sequence lengths but we do care about the final outputs\n",
    "final_outputs, _, _ = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths,\n",
    ")\n",
    "\n",
    "# Final class (word) probabilities are retrieved as the (sampled) softmax of the final outputs (decoder)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "# Build an Encoder-Decoder model\n",
    "#  - Note: Because the task is basically a classification task, we can use `sparse_categorical_crossentropy` as the loss function\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba],\n",
    ")\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Build a random sequence dataset\n",
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "# Train and validate the model on the random dataset\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-stone",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs\n",
    "For forecasting future values in a time series we want to have a *causal* model - a model in which future values are predicted solely on the basis of past values. On the other hand in NLP tasks (such as Neural Machine Translation) it can be beneficial to embed a word based on both the past and future contexts.\n",
    "\n",
    "A *Bidirectional* layer is a layer in which is composed of two layers working on the same input. One layer reads the input from the original direction (left to right) and the other one is a clone except it read from the reverse direction (right to left). The final output is some sort of a combination of both outputs - typically a concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "breeding-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_10 (GRU)                 (None, None, 10)          660       \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 20)          1320      \n",
      "=================================================================\n",
      "Total params: 1,980\n",
      "Trainable params: 1,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build an example RNN with a bidirectional GRU layer\n",
    "#  - `Bidirectional` wrapper creates a clone in the reverse direction of a layer passed as an argument and concatenates outputs\n",
    "#  -  Note: Adding a bidirectional wrapper implicitly doubles the number of units of the prototype\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "# Show model's topology\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-mainstream",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "Another improvement to predicting sequences of words is not to build single greedy model at a time but multiple. At each frame we keep a small set of $k$ most promising predictions (the *beam width*). In the next step we clone the model and compute new distribution over the vocabulary for the next word. But this time it's conditional probablity based on the previous word's probablity. We keep $k$ best sequence continuations based on $p(w_1 w_2) = p(w_2|w_1)*p(w_1)$ and iterate.\n",
    "\n",
    "Application of the *Beam Search* can limit the chance of producing words which are frequent in the training but sub-optimal (wrong) for particular sentence.\n",
    "\n",
    "```python\n",
    "beam_width = 10\n",
    "\n",
    "decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
    "    cell=decoder_cell,\n",
    "    beam_width=beam_width,\n",
    "    output_layer=output_layer,\n",
    ")\n",
    "\n",
    "final_outputs, _, _ = decoder(\n",
    "    decoder_embeddings,\n",
    "    start_tokens=start_tokens,\n",
    "    end_tokes=end_tokens,\n",
    "    initial_state=tfa.seq2seq.beam_search_decoder.tile_batch(encoder_state, multiplier=beam_width),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-circus",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "The main problem of RNNs is their short-term memory (even though cells like LSTM and GRU help). For instance in an Encoder-Decoder architecture for NMT, it still takes too many time steps for an information (word) to propagate from the encoder to the decoder. I.e. at the time the decoder tries to decode a word, it doen't know what the encoder thought of this word - it's lost the *attention*.\n",
    "\n",
    "The trick here is to add a shortcut - an *alignment model* (*attention model*) which takes in all the encoder outputs and combines them with decoder's hidden states to produce attention weights $\\alpha_{(t,i)}$ for the decoder (weights for the t-th decoder time step from i-th encoder output). These weights tell the decoder what to focus on.\n",
    "\n",
    "There three attention mechanisms, the former is the original one while the latter are typically performing better and are used nowadays:\n",
    "1. *Bahdanau attention (concatenative, additive)* - computes alphas by training them alongside the RNN by adding a time-distibuted dense layer feeding from concatenated `[endoder outputs; decoder hidden state]`, producing scores and applying a *softmax* (not time-distributed)\n",
    "1. *Luong attention (multiplicative)* - simplifies the mechanism by computing simple dot product between encoder's outputs and decoder's hidden state (scalar product is quite a successful similarity measure) instead of the dense layer to compute the scores; it also completely replaces decoder's previous hidden state by $\\tilde{\\mathbf{h}}_{(t)} = \\sum_i \\alpha_{(t,i)} \\mathbf{y}_i$.\n",
    "1. *Luong attention (general)* - is a somewhat a middle ground, it does add a simple linear transformation to encoder's outputs (dense layer without biases and activation) but otherwise it's *Luong's attention*.\n",
    "\n",
    "More formally, these mechanisms can be summarized as follows:\n",
    "$$\n",
    "\\tilde{\\mathbf{h}}_{(t)} = \\sum_i \\alpha_{(t,i)} \\mathbf{y}_i\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\alpha_{(t,i)} = \\frac{\\exp(e_{(t,i)})}{\\sum_{i'} \\exp(e_{(t,i')})}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "e_{(t,i)} = \\begin{cases}\n",
    "                \\mathbf{h}_{(t)}^T \\mathbf{y}_{(i)}                                   & \\quad \\text{dot}\\\\\n",
    "                \\mathbf{h}_{(t)}^T \\mathbf{W} \\mathbf{y}_{(i)}                        & \\quad \\text{general}\\\\\n",
    "                \\mathbf{v}^T \\tanh(\\mathbf{W}[\\mathbf{h}_{(t)}; \\mathbf{y}_{(i)}])  & \\quad \\text{concat}\n",
    "            \\end{cases}\n",
    "$$\n",
    "where $\\mathbf{v}$ is a rescaling parameter vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-oracle",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "The *Transformer* takes the attention mechanism to the next level and presents a deep net architecture based solely on thiese modules (a bit extended) that does not contain recurrent or conv. layers yet works as an Encoder-Decoder.\n",
    "\n",
    "As any Encoder-Decoder, it has two sides where the final output of the Encoder feeds into the hidden part of the Decoder:\n",
    "* The encoder part is fairly simple: it starts with imput embeddings, after which it adds *positional encoding* vectors (dense vectors that encode absolute and relative word positions in the input). Next there are *Multi Head Attention* and *Feed Forward* modules, each followed by a layer normalization and added skip connection from module inputs. The feed forward part are just two dense layers, the former with ReLU activations and the latter without any. Finally, this whole stack is repeated N times.\n",
    "* The decoder is basically the same but starts with a *Masked Multi Head Attention* which only differs in that it masks out inputs \"in the future\". Outputs of the encoder are fed to the middle (hidden) attention module. The decoder stack is also repeated N times.\n",
    "* The final decoder output (from the last layer of the last repetition) is passed through a simple linear layer with softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-darkness",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "As mentioned before, *Positional Encoding (PE)* is a dense vector encoding the word position in the input sequence which is added to the word embeddings. $PE_{p,i}$ is the i-th comonent (added to the i-th component of the word embedding) of the word located at p-th position in the sequence. The PE matric can be learned but it's typically pre-computed as a fixed encoding:\n",
    "$$\n",
    "PE_{p,i} = \\begin{cases}\n",
    "                \\sin(p / 10000^{i/d}) & \\quad \\text{if } i \\text{ is odd}\\\\\n",
    "                \\cos(p / 10000^{(i - 1)/d}) & \\quad \\text{if } i \\text{ is even}\n",
    "            \\end{cases}\n",
    "$$\n",
    "This fixed encoding is favoured because it has the same performance as learned and can extend to arbitrarily long sequences.\n",
    "\n",
    "TensorFlow does not have a `PositionalEncoding` layer but it's not hard to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "limiting-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    \"\"\"Positional encoding layer\"\"\"\n",
    "    \n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        \n",
    "        # Ensure that `max_dims` is even\n",
    "        if max_dims % 2 == 1:\n",
    "            max_dims += 1\n",
    "        \n",
    "        # Crate a space of possible positions and embedding indices\n",
    "        p, i = np.meshgrid(\n",
    "            np.arange(max_steps),\n",
    "            np.arange(max_dims // 2),\n",
    "        )\n",
    "        \n",
    "        # Precompute the maximum PE matrix using the formula presented above\n",
    "        pe = np.empty((1, max_steps, max_dims))\n",
    "        pe[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pe[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        \n",
    "        # Save the PE as the requested data type\n",
    "        self.positional_embedding = tf.constant(pe.astype(self.dtype))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Crop PE matrix to the shape of the inputs and add both together\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "\n",
    "\n",
    "# Very simplified version of the Transformer\n",
    "#  - Instead of Multi Head Attention uses plain Attention modules\n",
    "#  - Is missing skip connections\n",
    "#  - Omits layer normalization and dense nets\n",
    "\n",
    "# Hyperparameters of the model\n",
    "N = 6\n",
    "embed_size = 512\n",
    "max_steps = 500\n",
    "vocab_size = 10000\n",
    "\n",
    "# Define inputs for the two sides: encoder and decoder\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "# Define first layer - word embedding\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "# Add a Positional Encoding layer on top of embeddings\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)\n",
    "\n",
    "# Encoder stack\n",
    "Z = encoder_in\n",
    "for _ in range(N):\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "\n",
    "# Decoder stack\n",
    "#  - First attention module uses `causal=True`, i.e. masks out inputs \"from the future\"\n",
    "#  - Encoder outputs feed the second attention module\n",
    "Z = decoder_in\n",
    "for _ in range(N):\n",
    "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "# Network outputs one probability for each word in the vocabulary\n",
    "#  - Hence the dense layer of `vocab_size` units with softmax activation\n",
    "#  - Inpouts are the outputs of the very last layer of the decoder\n",
    "outputs = keras.layers.TimeDistributed(keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-resident",
   "metadata": {},
   "source": [
    "### Multi Head Attention\n",
    "The core component of a *Multi Head Attention* is a *Scaled Dot-Product* which was actually used in the example above (`use_scale=True`). The actual Multi Head Attention module is just a bunch of scaled do-product layers, each preceeded with three linear layers (time-distributed dense layer without activation; one for each $\\mathbf{V}, \\mathbf{K}, \\mathbf{Q}$ - presented below). Finally, all outputs of the scaled dot-product layers are concatenated and passed through a linear layer (again time-distributed).\n",
    "\n",
    "#### Scaled Dot Product\n",
    "Let's assume the encoder learns the meaning of words in a sentence - one can imagine this as a dictionary `\"They played chess ...\" -> {\"subject\": \"They\", \"verb\": \"played\", ...}`. The decoder then wants to do a lookup from this dictionary of, let's say, a `\"verb\"` - the issue is that we don't have discrete keys and values but rather vectorized representations of these.\n",
    "\n",
    "So instead of a lookup term we have a *query vector* $\\mathbf{q}$ and instead of a keys we have also a vector $\\mathbf{k}$. The dot product $\\mathbf{q}^T \\mathbf{k}$ is then a similarity score of how well the query matches the keys. If we pass it through a *softmax* (ensure it sums up to 1) and multiply the values $v$ we carry the relevance over from the key match to the values - i.e. query resutlts. The full scaled dot-product for a matrix of queries $\\mathbf{Q}$, keys $\\mathbf{K}$ and values $\\mathbf{V}$ is\n",
    "$$\n",
    "Attention(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = softmax (\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_{keys}}})\n",
    "$$\n",
    "where $\\sqrt{d_{keys}}$ is there to prevent saturating the softmax (tiny gradients). The code above actually lerarns this scaling factor but the Transformer uses this key dimention instead. \n",
    "\n",
    "Finally, the meaning of these matrices in the Encoder-Decoder setup is:\n",
    "* All the $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V}$ in the encoder equal to the list of words in an input sequence. So the encoder learns the relationships beween all pairs of words.\n",
    "* In the decoder masked it's pretty much the same - these correspond to the words in the target sentence but masked so that words don't compare to those after it.\n",
    "* Decoder's upper layers simply have $\\mathbf{K}$ and $\\mathbf{V}$ equal to the word encodings produced by the encoder while $\\mathbf{Q}$ is the word encodings produced by the decoder itself.\n",
    "\n",
    "#### The intuition behind Multi Head Attention\n",
    "The motivation behind using multiple heads (scaled dot-products) with preceeding linear layers is that a word encoding carries multiple information - about the word itself but also its position (due to PE) or e.g. past tense etc. The initial linear layers are there to make projections into these various sub-spaces, then we do the \"looup\" and finally project all these searches back with the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "transsexual-constraint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 50, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, n_heads, causal=False, use_scale=False, **kwargs):\n",
    "        self.n_heads = n_heads\n",
    "        self.causal = causal\n",
    "        self.use_scale = use_scale\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        self.dims = batch_input_shape[0][-1]\n",
    "        \n",
    "        # These could be hyperparameters instead\n",
    "        self.q_dims, self.v_dims, self.k_dims = [self.dims // self.n_heads] * 3\n",
    "        \n",
    "        # Build the initial Q, K and V linear layers for each head\n",
    "        self.q_linear = keras.layers.Conv1D(self.n_heads * self.q_dims, kernel_size=1, use_bias=False)\n",
    "        self.v_linear = keras.layers.Conv1D(self.n_heads * self.v_dims, kernel_size=1, use_bias=False)\n",
    "        self.k_linear = keras.layers.Conv1D(self.n_heads * self.k_dims, kernel_size=1, use_bias=False)\n",
    "        \n",
    "        # The attention part\n",
    "        self.attention = keras.layers.Attention(causal=self.causal, use_scale=self.use_scale)\n",
    "        \n",
    "        # Linear output layer\n",
    "        self.out_linear = keras.layers.Conv1D(self.dims, kernel_size=1, use_bias=False)\n",
    "        \n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def _multi_head_linear(self, inputs, linear):\n",
    "        shape = K.concatenate([K.shape(inputs)[:-1], [self.n_heads, -1]])\n",
    "        projected = K.reshape(linear(inputs), shape)\n",
    "        perm = K.permute_dimensions(projected, [0, 2, 1, 3])\n",
    "        return K.reshape(perm, [shape[0] * self.n_heads, shape[1], -1])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Split the inputs into Q, K and V\n",
    "        #  - K = V is not given in the inputs\n",
    "        q = inputs[0]\n",
    "        v = inputs[1]\n",
    "        k = inputs[2] if len(inputs) > 2 else v\n",
    "        \n",
    "        shape = K.shape(q)\n",
    "        \n",
    "        # Build the Q, K and V linear projections\n",
    "        q_proj = self._multi_head_linear(q, self.q_linear)\n",
    "        v_proj = self._multi_head_linear(v, self.v_linear)\n",
    "        k_proj = self._multi_head_linear(k, self.k_linear)\n",
    "        \n",
    "        # Pass these projections to the attention heads\n",
    "        multi_attended = self.attention([q_proj, v_proj, k_proj])\n",
    "        \n",
    "        # Reshape and concatenate the attention heads' outputs\n",
    "        shape_attended = K.shape(multi_attended)\n",
    "        reshaped_attended = K.reshape(multi_attended, [shape[0], self.n_heads, shape_attended[1], shape_attended[2]])\n",
    "        perm = K.permute_dimensions(reshaped_attended, [0, 2, 1, 3])\n",
    "        concat = K.reshape(perm, [shape[0], shape_attended[1], -1])\n",
    "        \n",
    "        # Finally apply project the outputs back with the last linear layer\n",
    "        return self.out_linear(concat)\n",
    "\n",
    "\n",
    "# Generate some random queries and values\n",
    "Q = np.random.rand(2, 50, 512)\n",
    "V = np.random.rand(2, 80, 512)\n",
    "\n",
    "# Test our Multi Head Attention module on these inputs\n",
    "multi_attn = MultiHeadAttention(8)\n",
    "multi_attn([Q, V]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-sunday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
