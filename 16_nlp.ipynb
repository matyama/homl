{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "allied-gross",
   "metadata": {},
   "source": [
    "# Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fatty-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected.\n"
     ]
    }
   ],
   "source": [
    "# FIXME: meke autocompletion working again\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if not physical_devices:\n",
    "    print(\"No GPU was detected.\")\n",
    "else:\n",
    "    # https://stackoverflow.com/a/60699372\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-popularity",
   "metadata": {},
   "source": [
    "## Char-RNN\n",
    "Let's build a RNN processing sequences of text and predicting single character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-dominant",
   "metadata": {},
   "source": [
    "### Loading the Data and Preparing the Dataset\n",
    "Following example uses famous Shakespear's texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conscious-smell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download the dataset\n",
    "filepath = keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    ")\n",
    "\n",
    "# Load raw dataset\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "    \n",
    "# Show a pice of the text\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "immune-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a character-based text tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regulation-submission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a text to a sequence of character IDs\n",
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hybrid-speed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a sequence of character IDs back to text\n",
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "moving-stick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "\n",
    "# total number of characters\n",
    "dataset_size = tokenizer.document_count\n",
    "\n",
    "# Encode the whole dataset\n",
    "#  - TF tokenizer assigns the first character it encounters with ID=1, we shift it back to start from 0\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "\n",
    "# Build a training TF Dataset from the first 90% of the text\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "# Preprocessing parameters\n",
    "# - length of a training instance (sequence of text)\n",
    "# - size of a training micro-batch\n",
    "n_steps = 100\n",
    "batch_size = 32\n",
    "\n",
    "# target = input shifted 1 character ahead\n",
    "window_length = n_steps + 1\n",
    "\n",
    "# Create training instances (sequences of text) by sliding a window over the text\n",
    "#  - each time we shift it by single character (`shift=1`)\n",
    "#  - `drop_remainder=True` means that we don't want to include final shortened windows with length < window length \n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "# Because `window()` creates a nested Dataset (containing sub-datasets), we want to flatten and convert it to single dataset of tensors\n",
    "#  - the trick here is that we batch the windows to the same length they already have\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Now we can safely shuffle the dataset and not to break the text\n",
    "#  - note: shuffling ensures some degree of i.i.d. which is necessary for SGD to work well\n",
    "#  - we also create training micro-batches\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "\n",
    "# Split the instances to (inputs, target) where the target is the next character\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "# As the last step we must either encode or embed categorical features (characters)\n",
    "#  - here we use 1-hot encoding since there's fairly few distinct characters\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# Finally we prefetch the data for better training performance\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Show shapes of 1st batch tensors\n",
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-columbus",
   "metadata": {},
   "source": [
    "### Creating and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conventional-eligibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 14s 223ms/step - loss: 3.4061\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 10s 223ms/step - loss: 2.9610\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 10s 224ms/step - loss: 2.6435\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 10s 222ms/step - loss: 2.4479\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 10s 223ms/step - loss: 2.3632\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 10s 227ms/step - loss: 2.2779\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 10s 228ms/step - loss: 2.2079\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 10s 219ms/step - loss: 2.1404\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 10s 222ms/step - loss: 2.0682\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 10s 218ms/step - loss: 2.0120\n"
     ]
    }
   ],
   "source": [
    "# Build a simple Char-RNN model:\n",
    "# - there are two GRU recurrent layers with 128 units, both of which use a 20% dropout (`recurrent_dropout`)\n",
    "# - there's also a 20% input dropout (`dropout` parameter of the 1st layer)\n",
    "# - the output layer is a time-distributed dense layer with 39 units and softmax activation to predict each character's class probability\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Train and validate the model for 10 epochs\n",
    "# - Note: This would take forever to train on my PC, so let's use just few batches\n",
    "history = model.fit(dataset.take(40), epochs=10)\n",
    "# history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-british",
   "metadata": {},
   "source": [
    "### Using the Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "south-darwin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(texts):\n",
    "    \"\"\"Preprocess given text to conform to Char-RNN's input\"\"\"\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "# Make a new prediction using the model\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "\n",
    "# Show the prediction as text: 1st sentence, last char\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-wheel",
   "metadata": {},
   "source": [
    "Next, let's generate not only single letter but whole new text. One approach is to repeatedly call the above. However, this often leads to repeating the same letter over and over again. Better approach is to select next letter randomly based on the learned class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "french-protest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "te and the beall the reake the belly the belly and \n"
     ]
    }
   ],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    \"\"\"\n",
    "    Generate new characters based on given text.\n",
    "     1. we pre-process and predict as before but return all character probablilities\n",
    "     2. then we compute the log of probabilities and scale it by the `temperature` parameter (the higher, the more in favour of higher prob. letters)\n",
    "     3. finally we select single character randomly given these log-probs. and convert the character ID back to text \n",
    "    \"\"\"\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    \"\"\"Extend given text with `n_chars` new letters\"\"\"\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Complete some text using different temperatures\n",
    "#  - Note: this example dosn't present the model very well since it's not been trained on the full dataset\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grave-domestic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucio. ar you up. greccoun:\n",
      "the beabudos the gile: \n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rotary-product",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ty no c't;\n",
      "meracqniogtt chai! aekgld arkichbrben; g\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-poster",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "The premise of a *Stateful RNN* is simple: So far we've thrown all neurons' hidden states away after applying BPTT on a training batch. In other words, hidden states were re-initialized for each partial update and so the model had hard time to learn long term patterns. The idea of a *Stateful RNN* is to keep the hidden state from previous batch and not to initialize it over again.\n",
    "\n",
    "This has, however, a consequence for the pre-processing logic. If we assume the state is transferred over from previous batches, these batches of training instances cannot overlap - they must consecutively extend each one. In our text generating example, this means we can't use overlapping windows and shuffling anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "middle-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 65s 199ms/step - loss: 2.9067\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 65s 209ms/step - loss: 2.2837\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 65s 209ms/step - loss: 2.6034\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 68s 217ms/step - loss: 2.6394\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 65s 209ms/step - loss: 2.4119\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 69s 219ms/step - loss: 2.2245\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 69s 221ms/step - loss: 2.1249\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 76s 242ms/step - loss: 2.0557\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 102s 325ms/step - loss: 2.0043\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 100s 320ms/step - loss: 1.9675\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 101s 323ms/step - loss: 1.9329\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 102s 327ms/step - loss: 1.9028\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 101s 323ms/step - loss: 1.8792\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 106s 338ms/step - loss: 1.8586\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 104s 334ms/step - loss: 1.8422\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 104s 331ms/step - loss: 1.8252\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 105s 337ms/step - loss: 1.8109\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 105s 334ms/step - loss: 1.7986\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 103s 328ms/step - loss: 1.7860\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 103s 328ms/step - loss: 1.7746\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 102s 325ms/step - loss: 1.7650\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 102s 325ms/step - loss: 1.7554\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 97s 309ms/step - loss: 1.7428\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 84s 269ms/step - loss: 1.7354\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 91s 289ms/step - loss: 1.7265\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 80s 255ms/step - loss: 1.7179\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 72s 231ms/step - loss: 1.7108\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 72s 229ms/step - loss: 1.7022\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 1.6947\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 61s 195ms/step - loss: 1.6889\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 60s 193ms/step - loss: 1.6812\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 60s 192ms/step - loss: 1.6759\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 59s 189ms/step - loss: 1.6705\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 64s 206ms/step - loss: 1.6649\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 81s 260ms/step - loss: 1.6596\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 68s 215ms/step - loss: 1.6537\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 71s 228ms/step - loss: 1.6486\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 75s 240ms/step - loss: 1.6442\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 71s 225ms/step - loss: 1.6392\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 73s 233ms/step - loss: 1.6355\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 85s 273ms/step - loss: 1.6302\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 67s 213ms/step - loss: 1.6253\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 73s 232ms/step - loss: 1.6219\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 74s 238ms/step - loss: 1.6202\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 70s 222ms/step - loss: 1.6150\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 64s 205ms/step - loss: 1.6127\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 69s 221ms/step - loss: 1.6089\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 68s 218ms/step - loss: 1.6060\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 65s 208ms/step - loss: 1.6027\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 64s 204ms/step - loss: 1.5994\n"
     ]
    }
   ],
   "source": [
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# (a) Updated pre-processing logic for Stateful Char-RNN\n",
    "# - In this version we apply single window at a time\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "# Contrary to before, we shift windows by full `n_steps` to create non-overlapping inputs\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# We skip shuffling altogether so that we don't break the preserved state and batch by 1\n",
    "#  - batching by 1 means that we apply just single window at a time and, again, preserve the state\n",
    "dataset = dataset.repeat().batch(1)\n",
    "\n",
    "# The rest of the logic is analogous\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# (b) Updated pre-processing logic for Stateful Char-RNN\n",
    "# - In this more complicated version we apply a micro-batch of windows as before\n",
    "batch_size = 32\n",
    "\n",
    "@tf.function\n",
    "def make_windowed_ds(encoded_part):\n",
    "    \"\"\"Creates a flat windowed TF Dataset of non-overlapping windows\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    return dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Contrary to before, we make a windowed Dataset in two steps:\n",
    "#  1. We split the dateset into equal length batches and make windowed Dataset from each batch\n",
    "#  2. Then we put put all these batches back together and stack the windows so that \n",
    "#     the n-th inputs sequence of a batch starts where the n-th sequence of the previous one ended\n",
    "datasets = map(make_windowed_ds, np.array_split(encoded[:train_size], batch_size))\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "\n",
    "# Final steps are the same:\n",
    "#  - Split each window to (inputs, target)\n",
    "#  - 1-hot encode the categorical input features\n",
    "#  - Prefetch the data for better performance\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Build a Stateful RNN model\n",
    "# The architecture is basically the same as before, notice two distinctions:\n",
    "#  - `stateful=True` on the recurrent layers to preserve hidden state\n",
    "#  - `batch_input_shape` set for the initial recurrent layer to let the model know the shape (batch size) for the hidden state\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(\n",
    "        128,\n",
    "        return_sequences=True,\n",
    "        stateful=True,\n",
    "        dropout=0.2,\n",
    "        recurrent_dropout=0.2,\n",
    "        batch_input_shape=[batch_size, None, max_id],\n",
    "    ),\n",
    "    keras.layers.GRU(\n",
    "        128, \n",
    "        return_sequences=True,\n",
    "        stateful=True,\n",
    "        dropout=0.2,\n",
    "        recurrent_dropout=0.2,\n",
    "    ),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\")),\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Train and validate the model\n",
    "#  - we use custom callback to reset model's state at the start of each epoch (instead of each batch)\n",
    "#  - we train the model for 50 epochs, also notice the updated `steps_per_epoch`\n",
    "\n",
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Callback that resets model's state each epoch\"\"\"\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    dataset, \n",
    "    steps_per_epoch=train_size // batch_size // n_steps,\n",
    "    epochs=50,\n",
    "    callbacks=[ResetStatesCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-begin",
   "metadata": {},
   "source": [
    "To use the model with different batch sizes, we need to create a stateless copy. We can get rid of dropout since it is only used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "buried-complex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the court,\n",
      "when they shortime she down.\n",
      "peserve, ab\n"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a steteless Char-RNN model\n",
    "# - This model is based on our steteful Char-RNN but used only for making predictions\n",
    "# - Notice: We don't need dropout since it's used only during training\n",
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\")),\n",
    "])\n",
    "\n",
    "# Build the stateless model\n",
    "#  - Firstly, we can loosen the fixed batch size restriction\n",
    "#  - Secondly, we copy learned weights from the stateful model (this works fine since dropout layers have no trainable params)\n",
    "stateless_model.build(tf.TensorShape([None, None, max_id]))\n",
    "stateless_model.set_weights(model.get_weights())\n",
    "\n",
    "# Replace our main model by this one\n",
    "#  - because `complete_text()` implicitly works with `model`\n",
    "model = stateless_model\n",
    "\n",
    "# Try to complete some text\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-martial",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "Let's take a step further from the character-level RNNs to word-level sentiment analysis. Typical dataset from this taks is the IMDb reviews dataset, so let's play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cardiac-soviet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Load the IMDb reviews dataset\n",
    "(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()\n",
    "\n",
    "# Show a training instance\n",
    "#  - The dataset is already preprocessed, each instance is a sequence integers which represent an ID of a word\n",
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "graphic-copper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to reconstruct a word we can load the word to ID index\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# And then create an inverse mapping\n",
    "# - Note: We shift the ID by 3 to reserve first three IDs for special markers\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "\n",
    "# These special markers are for the:\n",
    "#  - padding symbol\n",
    "#  - start of sequence\n",
    "#  - unknown word\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "    \n",
    "# Show a sample of decoded words\n",
    "\" \".join(id_to_word[id_] for id_ in X_train[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-albert",
   "metadata": {},
   "source": [
    "Now, let's create the same pre-processing logic and trainable dataset using TensorFlow's Datasets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "limited-catholic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the IMDb reviews TF Dataset\n",
    "#  - Note: Using TF-only functions allows us to reuse the same pre-processing logic in every environment\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "\n",
    "# List the dataset content\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "photographic-cream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save and show training and test set sizes\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples\n",
    "\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "concrete-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peek the training dataset\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "involved-smell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    \"\"\"\n",
    "    Pre-process an input batch:\n",
    "     1. Crops each instance to first 300 characters (speeds up training and sentiment can usually be deduced by the first few sentences)\n",
    "     2. Replaces '<br />' symbols by a space character\n",
    "     3. Replaces each non-letter and quote character by a space\n",
    "     4. Splits instances by space creating a ragged tensor\n",
    "     5. Returns a dense tensor (and original label) made by padding the splits with '<pad>'\n",
    "    \"\"\"\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
    "\n",
    "# Try the preprocessing logic on the first training batch\n",
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "removable-smart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Do a word-count over the whole pre-processed training dataset (in one pass)\n",
    "vocabulary = Counter(\n",
    "    word.numpy()\n",
    "    for X_batch, _ in datasets[\"train\"].batch(batch_size).map(preprocess)\n",
    "    for review in X_batch\n",
    "    for word in review\n",
    ")\n",
    "\n",
    "# Show first 3 most common words in the training corpus\n",
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "chemical-symphony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "atomic-publicity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "12\n",
      "11\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Drop the least important words and keep just 10k most frequent ones\n",
    "vocab_size = 10_000\n",
    "truncated_vocabulary = [word for word, _ in vocabulary.most_common(vocab_size)]\n",
    "\n",
    "# Make a word index from the truncated vocabulary\n",
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "\n",
    "# Test the word index on an example sentence\n",
    "for word in b\"This movie was faaaaaantastic\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "interracial-committee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a static vocabulary table with 1k OOV buckets\n",
    "num_oov_buckets = 1000\n",
    "\n",
    "# Initialize the vocabulary from our truncated vocabulary and word index\n",
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "\n",
    "# Build the lookup table\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "# Test the lookup table on the example sentence we used before\n",
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "checked-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    \"\"\"Encode each word in an input batch using the static vocabulary table\"\"\"\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "# Preprocess and encode the whole training set\n",
    "train_set = (\n",
    "    datasets[\"train\"]\n",
    "    .repeat()\n",
    "    .batch(batch_size)\n",
    "    .map(preprocess)\n",
    "    .map(encode_words)\n",
    "    .prefetch(1)\n",
    ")\n",
    "\n",
    "# Display the 1st training batch\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "positive-montana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 100s 119ms/step - loss: 0.5957 - accuracy: 0.6606\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 92s 117ms/step - loss: 0.3701 - accuracy: 0.8398\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 91s 117ms/step - loss: 0.2081 - accuracy: 0.9237\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 92s 118ms/step - loss: 0.1412 - accuracy: 0.9512\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 93s 119ms/step - loss: 0.1072 - accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "# The embedding dimention hyperparameter\n",
    "embed_size = 128\n",
    "\n",
    "# Build a classification RNN with initial word embedding layer\n",
    "#  - This layer's matrix has shape [ID count = vocabulary size + OOV buckets, embedding dimension]\n",
    "#  - So the model's inputs are 2D tensors of shape [batch size, time steps], the embedding output is 3D tensor [batch size, time steps, embedding size]\n",
    "#  - `mask_zero=True` means that we ignore ID=0 - the most frequent word which in our case is `<pad>` (so the model doesn't have to learn to ignore it)\n",
    "#  - note: It would clearner to ensure that the padding word really has ID 0 than to count on the fact that it's the most frequent one.\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, mask_zero=True, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train and validate the model for 5 epochs\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-copper",
   "metadata": {},
   "source": [
    "### Manual Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "republican-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 98s 117ms/step - loss: 0.6093 - accuracy: 0.6406\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 93s 119ms/step - loss: 0.3711 - accuracy: 0.8425\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 94s 121ms/step - loss: 0.1953 - accuracy: 0.9286\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 93s 119ms/step - loss: 0.1205 - accuracy: 0.9582\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 94s 120ms/step - loss: 0.1056 - accuracy: 0.9631\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "\n",
    "# Define an input layer\n",
    "inputs = keras.layers.Input(shape=[None])\n",
    "\n",
    "# Create a mask that ignores inputs equal to 0\n",
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "\n",
    "# Build the same model structure as before but with explicit masking of layer inputs\n",
    "#  - Note: In the previous example the output dense layer didn't receive the implicit mask because the time dimension was not the same, \n",
    "#          so the explicit masking is necessary if we want to propagate this information all the way to the loss function.\n",
    "#  - Note 2: The downside is that LSTMs and GRUs won't use optimized impl. for GPUs and so the training might be slower.\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "\n",
    "# Define model's outputs\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "# Compose and compile the model\n",
    "model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train and validate the model for 5 epochs\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-america",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "standard-finding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 3s 4ms/step - loss: 0.5861 - accuracy: 0.6919\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 3s 4ms/step - loss: 0.5181 - accuracy: 0.7445\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 3s 4ms/step - loss: 0.5122 - accuracy: 0.7494\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 3s 4ms/step - loss: 0.5086 - accuracy: 0.7492\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 3s 3ms/step - loss: 0.5052 - accuracy: 0.7518\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build a model with pre-trained layers:\n",
    "#  - Main portion of this model reuses Google's model that pre-processes and embeds words from an input text to 50 dimensional vectors\n",
    "#  - Then we just add two dense layers for our classification task of sentiment analysis\n",
    "#  - Note: By default TF Hub downloads models to /tmp, one can override this by setting `TFHUB_CACHE_DIR` env. variable\n",
    "#  - Note 2: TF Hub layers are also by default non-trainable - if we want to tweak their weights we must unfreeze them\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "        dtype=tf.string,\n",
    "        input_shape=[],\n",
    "        output_shape=[50],\n",
    "    ),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Then we can just load the IMDb reviews dataset\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "\n",
    "# Take the training set and just batch it (and prefetch)\n",
    "#  - Note: The rest of the preprocessing logic is handled by the TF Hub portion of the model\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
    "\n",
    "# Finally we just train and validate the model on our IMDb dataset\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-header",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Network for Neural Machine Translation\n",
    "\n",
    "As the name suggests, in the *Encoder-Decoder* architecture we split a *sequence-to-sequence* RNN into two parts:\n",
    "1. Encoder - takes as inputs reversed sequences of words (or rather embeddings thereof; reversed so that the decoder reveives the first word first)\n",
    "1. Decoder - this part has actually two inputs, first the hidden states of the encoder and socond is either previous target word (during training; embedded) or the actual token that was output in the previous step (during inference; embedded)\n",
    "\n",
    "Additional notes to the architecture:\n",
    "* The outputs of the decoder are scores for each word in the vocabulary which are turned to probabilities using time-distributed *softmax*. Because we can easily get to very high-dimensional outputs, typically a *sampled softmax* is used for training and regular *softmax* for inference\n",
    "* In this task we cannot simply truncate input sequences to common length as before because we want to get complete translations. Also pedding to some large common lenght does not work. Instead, we can bucket the sentenced into sets of close-enough lenght and pad these to match the longes one in each set.\n",
    "* Finally, we should ignore part of the output after an `<EOS>` token - both from the output and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "suburban-arena",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 9s 169ms/step - loss: 4.6052\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 6s 171ms/step - loss: 4.6024\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Set the RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Sutup vocabulary and embedding size hyperparameters\n",
    "vocab_size = 100\n",
    "embed_size = 10\n",
    "\n",
    "# Define Encoder and Decoder inputs\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "# Create embedding layers for the Encoder and Decoder parts\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "# Encoder is a 512 unit LSTM layer\n",
    "#  - we can ignore encoder ouputs but we return both the short-term and long-term states with `return_state=True`\n",
    "#  - the complete hidden state of the encoder is a pair of the short and long-term states\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "_, state_h, state_c = encoder(encoder_embeddings)\n",
    "\n",
    "# Decoder is based on the `BasicDecoder` from TF Addons\n",
    "#  - Decoder cell is a 512 unit LSTM cell\n",
    "#  - Sampler is a component tells the Decoder what it should pretend the last step's output was:\n",
    "#    - in this case `TrainingSampler` takses the embedding of previous target token\n",
    "#    - other option is `ScheduledEmbedingTrainingSampler` which randomly chooses between target and actual outputs\n",
    "#  - Model's output is a dense layer with one unit per word in the vocabulary\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    cell=keras.layers.LSTMCell(512),\n",
    "    sampler=tfa.seq2seq.sampler.TrainingSampler(),\n",
    "    output_layer=keras.layers.Dense(vocab_size),\n",
    ")\n",
    "\n",
    "# Construct the Decoder\n",
    "#  - Initial state is the complete encoder state\n",
    "#  - We can ignore final decoder state and sequence lengths but we do care about the final outputs\n",
    "final_outputs, _, _ = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=[state_h, state_c],\n",
    "    sequence_length=sequence_lengths,\n",
    ")\n",
    "\n",
    "# Final class (word) probabilities are retrieved as the (sampled) softmax of the final outputs (decoder)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "# Build an Encoder-Decoder model\n",
    "#  - Note: Because the task is basically a classification task, we can use `sparse_categorical_crossentropy` as the loss function\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba],\n",
    ")\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Build a random sequence dataset\n",
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "# Train and validate the model on the random dataset\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-allowance",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs\n",
    "For forecasting future values in a time series we want to have a *causal* model - a model in which future values are predicted solely on the basis of past values. On the other hand in NLP tasks (such as Neural Machine Translation) it can be beneficial to embed a word based on both the past and future contexts.\n",
    "\n",
    "A *Bidirectional* layer is a layer in which is composed of two layers working on the same input. One layer reads the input from the original direction (left to right) and the other one is a clone except it read from the reverse direction (right to left). The final output is some sort of a combination of both outputs - typically a concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "preliminary-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_10 (GRU)                 (None, None, 10)          660       \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 20)          1320      \n",
      "=================================================================\n",
      "Total params: 1,980\n",
      "Trainable params: 1,980\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build an example RNN with a bidirectional GRU layer\n",
    "#  - `Bidirectional` wrapper creates a clone in the reverse direction of a layer passed as an argument and concatenates outputs\n",
    "#  -  Note: Adding a bidirectional wrapper implicitly doubles the number of units of the prototype\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "# Show model's topology\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-drunk",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
