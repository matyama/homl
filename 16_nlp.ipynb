{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worst-saying",
   "metadata": {},
   "source": [
    "# Natural Language Processing with RNNs and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "isolated-accessory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected.\n"
     ]
    }
   ],
   "source": [
    "# FIXME: meke autocompletion working again\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if not physical_devices:\n",
    "    print(\"No GPU was detected.\")\n",
    "else:\n",
    "    # https://stackoverflow.com/a/60699372\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-pennsylvania",
   "metadata": {},
   "source": [
    "## Char-RNN\n",
    "Let's build a RNN processing sequences of text and predicting single character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-policy",
   "metadata": {},
   "source": [
    "### Loading the Data and Preparing the Dataset\n",
    "Following example uses famous Shakespear's texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liberal-papua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download the dataset\n",
    "filepath = keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    ")\n",
    "\n",
    "# Load raw dataset\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "    \n",
    "# Show a pice of the text\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "documentary-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a character-based text tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dated-reason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a text to a sequence of character IDs\n",
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "binding-samba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a sequence of character IDs back to text\n",
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "piano-print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "# Set RNG state\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "\n",
    "# total number of characters\n",
    "dataset_size = tokenizer.document_count\n",
    "\n",
    "# Encode the whole dataset\n",
    "#  - TF tokenizer assigns the first character it encounters with ID=1, we shift it back to start from 0\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "\n",
    "# Build a training TF Dataset from the first 90% of the text\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "# Preprocessing parameters\n",
    "# - length of a training instance (sequence of text)\n",
    "# - size of a training micro-batch\n",
    "n_steps = 100\n",
    "batch_size = 32\n",
    "\n",
    "# target = input shifted 1 character ahead\n",
    "window_length = n_steps + 1\n",
    "\n",
    "# Create training instances (sequences of text) by sliding a window over the text\n",
    "#  - each time we shift it by single character (`shift=1`)\n",
    "#  - `drop_remainder=True` means that we don't want to include final shortened windows with length < window length \n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "# Because `window()` creates a nested Dataset (containing sub-datasets), we want to flatten and convert it to single dataset of tensors\n",
    "#  - the trick here is that we batch the windows to the same length they already have\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Now we can safely shuffle the dataset and not to break the text\n",
    "#  - note: shuffling ensures some degree of i.i.d. which is necessary for SGD to work well\n",
    "#  - we also create training micro-batches\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "\n",
    "# Split the instances to (inputs, target) where the target is the next character\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "# As the last step we must either encode or embed categorical features (characters)\n",
    "#  - here we use 1-hot encoding since there's fairly few distinct characters\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# Finally we prefetch the data for better training performance\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Show shapes of 1st batch tensors\n",
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-objective",
   "metadata": {},
   "source": [
    "### Creating and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bound-particle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 9s 143ms/step - loss: 3.4060\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 7s 148ms/step - loss: 2.9614\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 7s 158ms/step - loss: 2.6444\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 7s 165ms/step - loss: 2.4473\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 8s 172ms/step - loss: 2.3629\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 8s 175ms/step - loss: 2.2824\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 9s 192ms/step - loss: 2.2088\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 8s 184ms/step - loss: 2.1406\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 8s 186ms/step - loss: 2.0690\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 9s 203ms/step - loss: 2.0117\n"
     ]
    }
   ],
   "source": [
    "# Build a simple Char-RNN model:\n",
    "# - there are two GRU recurrent layers with 128 units, both of which use a 20% dropout (`recurrent_dropout`)\n",
    "# - there's also a 20% input dropout (`dropout` parameter of the 1st layer)\n",
    "# - the output layer is a time-distributed dense layer with 39 units and softmax activation to predict each character's class probability\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# Train and validate the model for 10 epochs\n",
    "# - Note: This would take forever to train on my PC, so let's use just few batches\n",
    "history = model.fit(dataset.take(40), epochs=10)\n",
    "# history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-interval",
   "metadata": {},
   "source": [
    "### Using the Model to Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "victorian-sierra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: this example dosn't present the model very well since it's not been trained on the full dataset (see previous cell)\n",
    "\n",
    "def preprocess(texts):\n",
    "    \"\"\"Preprocess given text to conform to Char-RNN's input\"\"\"\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "# Make a new prediction using the model\n",
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "\n",
    "# Show the prediction as text: 1st sentence, last char\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-community",
   "metadata": {},
   "source": [
    "Next, let's generate not only single letter but whole new text. One approach is to repeatedly call the above. However, this often leads to repeating the same letter over and over again. Better approach is to select next letter randomly based on the learned class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "identical-commons",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t the bell and in the belly the belly and the the b\n"
     ]
    }
   ],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    \"\"\"\n",
    "    Generate new characters based on given text.\n",
    "     1. we pre-process and predict as before but return all character probablilities\n",
    "     2. then we compute the log of probabilities and scale it by the `temperature` parameter (the higher, the more in favour of higher prob. letters)\n",
    "     3. finally we select single character randomly given these log-probs. and convert the character ID back to text \n",
    "    \"\"\"\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    \"\"\"Extend given text with `n_chars` new letters\"\"\"\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Complete some text using different temperatures\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "union-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tucio' thenf'th,\n",
      "affed, you the beagu, as le gileve\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "identified-psychology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ty no c't;\n",
      "meracqniogtt cino! aekfll ar:hwigh: n: b\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-connecticut",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
