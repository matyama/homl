{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning and Random Forests\n",
    "\n",
    "## Voting Classifiers\n",
    "* Train multiple classifiers on the training set (possibly the same or subsets - more later) and construct an ensemble classifier by e.g. a mode (the most frequent output)\n",
    "* Typically performs better than individual classifiers - follows from the law of large numbers (the more classifiers the better)\n",
    "* But using the same training set is problematic as individual classifiers might be correlated (not iid) - classifiers will learn the same mistakes, especially when all implement the same learning algorithm\n",
    "* Regression works similarly and typically uses mean instead of statistical mode for the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.839\n",
      "RandomForestClassifier 0.8615\n",
      "SVC 0.868\n",
      "VotingClassifier 0.868\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples=10_000, noise=0.4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svc_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svc_clf)],\n",
    "    voting='hard',\n",
    ")\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svc_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "* The idea here is to prevent correlation by splitting the training set into subsets and training each predictor (all of one type) on one subset\n",
    "* When the subset sampling is done *with replacement* then we call this technique **bagging** (in statistics also called **bootstrap**), when it's done *without replacement* we call it **pasting**\n",
    "* Aggregate model should in theory generalize better - the reasoning here is that we trade increased bias (because we use many models of the same type) for lower variance (because of splitting the training set and using aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8705"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "Because bootstrap samples (with replacement) out of all $m$ training just some into each taining sub-set it means that only about 63% on average of all the training instances will sampled for some predictor. The rest, which is never sampled, is called *out-of-bag* instances.\n",
    "\n",
    "The idea here is that we can use OOB instances as a free validation set because it is likely that each training instance will be OOB for several estimators. *Scikit-Learn* can automatically collect OOB score during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83875"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True,\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.03684211, 0.96315789],\n",
       "       [0.6344086 , 0.3655914 ],\n",
       "       ...,\n",
       "       [0.92021277, 0.07978723],\n",
       "       [0.89784946, 0.10215054],\n",
       "       [0.15425532, 0.84574468]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspaces\n",
    "\n",
    "Another variant (or rather extension) of a bagging classifier is sampling features as well as training instances.\n",
    "* *Random patches* - sample both features and instances\n",
    "* *Random subspaces* - sample features but keep all instance (set `bootstrap=False, max_samples=1.0`, `bootstrap_features=True` and/or `max_features` to less than 1\n",
    "\n",
    "In *Scikit-Learn* one can control feature sampling with `bootstrap_features` and `max_features` which works similarly to `bootstrap` and `max_samples`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "In *Scikit-Learn* it's basically a convenience API for a begging (sometimes pasting) classifier/regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8675"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# Equivalent to:\n",
    "# bag_clf = BaggingClassifier(\n",
    "#     base_estimator=DecisionTreeClassifier(max_features='auto', max_leaf_nodes=16),\n",
    "#     n_estimators=500,\n",
    "#     max_samples=1.0,\n",
    "#     bootstrap=True,\n",
    "#     n_jobs=-1,\n",
    "# )\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Random forests have a nice property that they can easily estimate feature importance by looking at how much the tree nodes using a feature reduce impurity on average (over all trees in the forest, weighted by the leaf sample size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09150843011552325\n",
      "sepal width (cm) 0.022125152106630883\n",
      "petal length (cm) 0.4380628768059563\n",
      "petal width (cm) 0.4483035409718895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Technique which trains a strong predictor by sequentially improving weak predictors. Typical eamples are:\n",
    "* **AdaBoos** - adaptive boosting, each training sample is assigned a weight - base classifiers are trained on these weighted samples and combined (weighted) into final predictor\n",
    "* **Gradient Boosting** - similar to *AdaBoost* but consecutive weak predictors (after the first one) are trained on residual errors of the previous one, final predictor is simple sum of all weak ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "The main idea is to sequentially train weak predictors and combine them into strong one. Each training sample is assigned a weight $w^{(i)}$ initialized to $1/m$. The weighted error rate of $j^{th}$ predictor is\n",
    "$\n",
    "r_j = \\frac{\\sum_{i = 1, \\hat{y}_j^{(i)} = y^{(i)}}^m w^{(i)}}{\\sum_{i = 1}^m w^{(i)}}\n",
    "$\n",
    "\n",
    "Next, each predictor is assigned a weight $\\alpha_j$ computed from it's error rate ($\\eta$ is a learning rate hyper-parameter): $\\alpha_j = \\eta \\log{\\frac{1 - r_j}{r_j}}$.\n",
    "\n",
    "The iteration is concluded by updating sample weights as follows: \n",
    "$\n",
    "w^{(i)} = \n",
    "    \\begin{cases}\n",
    "       w^{(i)} & \\quad \\text{if } \\hat{y}_j^{(i)} = y^{(i)}\\\\\n",
    "       w^{(i)}\\exp(\\alpha_j) & \\quad \\text{if } \\hat{y}_j^{(i)} \\ne y^{(i)}\n",
    "    \\end{cases}\n",
    "$\n",
    "\n",
    "Final predictor is combined from all intermediate predictors as $\\hat{y}(\\mathbf{x}) = \\text{argmax}_k \\sum_{j = 1, \\hat{y}_j(\\mathbf{x}) = k}^N \\alpha_j$ where $N$ is the number of iterations (weak predictors).\n",
    "\n",
    "*Scikit-Learn* by default uses *Decision Stumps* as base predictors - decision trees with single decision node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=200,\n",
    "    algorithm='SAMME.R',\n",
    "    learning_rate=0.5,\n",
    ")\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "As described above, the idea is quite simple:\n",
    "1. train initial predictor on the training set targets\n",
    "2. train consecutive predictors to target the residual errors of the previous predictor\n",
    "3. after convergence return a combination (sum) of all predictors in the sequence\n",
    "\n",
    "This is how one would do it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel('$x_1$', fontsize=18)\n",
    "plt.ylabel('$y$', rotation=0, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "X_new = np.array([[0.8]])\n",
    "\n",
    "y_pred = np.sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)\n",
    "\n",
    "gbrt.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can use early stopping strategy to find the best number of estimators. First version below generates all the trees (estimators) first and then searches for the best setting whereas the next version implements real early stopping - does not generate more trees if the validation error does not drop for some number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=85)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "best_n = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=best_n)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=69, warm_start=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "max_steps_going_up = 5\n",
    "\n",
    "min_val_error = float('inf')\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == max_steps_going_up:\n",
    "            break  # early stopping\n",
    "            \n",
    "gbrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "*XGBoost* is specialized library for gradient boosting with API similar to *Scikit-Learn*. One of many nice things about it is that it implements early stopping out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.22055\n",
      "[1]\tvalidation_0-rmse:0.16547\n",
      "[2]\tvalidation_0-rmse:0.12243\n",
      "[3]\tvalidation_0-rmse:0.10044\n",
      "[4]\tvalidation_0-rmse:0.08467\n",
      "[5]\tvalidation_0-rmse:0.07344\n",
      "[6]\tvalidation_0-rmse:0.06728\n",
      "[7]\tvalidation_0-rmse:0.06383\n",
      "[8]\tvalidation_0-rmse:0.06125\n",
      "[9]\tvalidation_0-rmse:0.05959\n",
      "[10]\tvalidation_0-rmse:0.05902\n",
      "[11]\tvalidation_0-rmse:0.05852\n",
      "[12]\tvalidation_0-rmse:0.05844\n",
      "[13]\tvalidation_0-rmse:0.05801\n",
      "[14]\tvalidation_0-rmse:0.05747\n",
      "[15]\tvalidation_0-rmse:0.05772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0033024085351185672"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg = XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "mean_squared_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "The idea behind stacking is instead of hard voting tain another predictor to aggregate the predictors in an ensemble. Tere are two approaches how to train the ensemble:\n",
    "1. Split the training set into two sub-sets, first is used to train base predictors, the other (*hold-out set*) is used to create predictions of base estimators to create a training set for the *blender* (this is called **blending**)\n",
    "2. Alternatively, it is possible to use *out-of-fold* predictions - this is called **stacking**\n",
    "\n",
    "Note that this approach can be easily extended to more than just one layer by splitting the training set into multiple times and repeating what's described above. *Scikit-Learn* does not implement this out of the box but there are good libraries for it - e.g. [DESlib](https://github.com/scikit-learn-contrib/DESlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(mnist.data, mnist.target, test_size=10_000, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=10_000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = LinearSVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the RandomForestClassifier(random_state=42)\n",
      "Training the ExtraTreesClassifier(random_state=42)\n",
      "Training the LinearSVC(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf]\n",
    "\n",
    "for estimator in estimators:\n",
    "    print('Training the', estimator)\n",
    "    estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9692, 0.9715, 0.8695]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_val, y_val) for estimator in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest_clf',\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             ('extra_trees_clf',\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             ('svm_clf', LinearSVC(random_state=42))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [\n",
    "        (\"random_forest_clf\", random_forest_clf),\n",
    "        (\"extra_trees_clf\", extra_trees_clf),\n",
    "        (\"svm_clf\", svm_clf),\n",
    "    ]\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9692, 0.9715, 0.8695]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_val, y_val) for _, estimator in voting_clf.estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it without the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('random_forest_clf',\n",
       "                              RandomForestClassifier(random_state=42)),\n",
       "                             ('extra_trees_clf',\n",
       "                              ExtraTreesClassifier(random_state=42)),\n",
       "                             ('svm_clf', None)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.set_params(svm_clf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('random_forest_clf', RandomForestClassifier(random_state=42)),\n",
       " ('extra_trees_clf', ExtraTreesClassifier(random_state=42)),\n",
       " ('svm_clf', None)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(random_state=42),\n",
       " ExtraTreesClassifier(random_state=42),\n",
       " LinearSVC(random_state=42)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "del voting_clf.estimators_[2]\n",
    "del voting_clf.estimators[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9713"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9719"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9645, 0.9691]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[estimator.score(X_test, y_test) for _, estimator in voting_clf.estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 5.],\n",
       "       [8., 8., 8.],\n",
       "       [2., 2., 2.],\n",
       "       ...,\n",
       "       [7., 7., 7.],\n",
       "       [6., 6., 6.],\n",
       "       [7., 7., 7.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for i, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, i] = estimator.predict(X_val)\n",
    "    \n",
    "X_val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9698"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for i, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, i] = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9665"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
