{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "antique-halifax",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-level",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradient Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "single-wildlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the input space\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "# Plot the Logit activation function (Sigmoid)\n",
    "plt.plot([-5, 5], [0, 0], \"k-\")\n",
    "plt.plot([-5, 5], [1, 1], \"k--\")\n",
    "plt.plot([0, 0], [-0.2, 1.2], \"k-\")\n",
    "\n",
    "plt.plot([-5, 5], [-3/4, 7/4], \"g--\")\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-basin",
   "metadata": {},
   "source": [
    "### Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "local-watch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial-keyboard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f81ef9c3a00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Layer with ReLU activation and He normal initialization\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "manual-latin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f81eeabb340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The default mode for this initialization is `fan_in`\n",
    "#  - to switch to `fan_avg` one has to use `VarianceScaling`\n",
    "init = keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-portal",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-underground",
   "metadata": {},
   "source": [
    "#### Leaky ReLU\n",
    "$\\text{LeakyReLU}_\\alpha(z) = \\max(\\alpha z, z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ready-citizenship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "# Plot the LeakyReLU activation function\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interracial-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "breeding-asset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mobile-species",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 1.6314 - accuracy: 0.5054 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.8416 - accuracy: 0.7246 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.7053 - accuracy: 0.7637 - val_loss: 0.6427 - val_accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.6325 - accuracy: 0.7908 - val_loss: 0.5900 - val_accuracy: 0.8066\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5992 - accuracy: 0.8019 - val_loss: 0.5582 - val_accuracy: 0.8202\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5624 - accuracy: 0.8141 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5379 - accuracy: 0.8218 - val_loss: 0.5157 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5152 - accuracy: 0.8295 - val_loss: 0.5079 - val_accuracy: 0.8282\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5100 - accuracy: 0.8268 - val_loss: 0.4895 - val_accuracy: 0.8386\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4918 - accuracy: 0.8339 - val_loss: 0.4817 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "# Load, split and scale the Fashion MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "max_value = 255.0\n",
    "val_split = 5000\n",
    "X_train_full = X_train_full / max_value\n",
    "X_test = X_test / max_value\n",
    "X_valid, X_train = X_train_full[:val_split], X_train_full[val_split:]\n",
    "y_valid, y_train = y_train_full[:val_split], y_train_full[val_split:]\n",
    "\n",
    "# Set RNG state\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build a model with LeakyReLU activations and He normal initialization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train and validate the model for 10 epochs\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-portland",
   "metadata": {},
   "source": [
    "#### PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "annual-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 10s 5ms/step - loss: 1.6969 - accuracy: 0.4974 - val_loss: 0.9255 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.8706 - accuracy: 0.7247 - val_loss: 0.7305 - val_accuracy: 0.7632\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.7211 - accuracy: 0.7620 - val_loss: 0.6565 - val_accuracy: 0.7878\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.6448 - accuracy: 0.7880 - val_loss: 0.6004 - val_accuracy: 0.8046\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.6078 - accuracy: 0.8003 - val_loss: 0.5656 - val_accuracy: 0.8180\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5693 - accuracy: 0.8119 - val_loss: 0.5407 - val_accuracy: 0.8240\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5428 - accuracy: 0.8193 - val_loss: 0.5196 - val_accuracy: 0.8314\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5193 - accuracy: 0.8283 - val_loss: 0.5113 - val_accuracy: 0.8316\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5129 - accuracy: 0.8274 - val_loss: 0.4916 - val_accuracy: 0.8378\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4941 - accuracy: 0.8314 - val_loss: 0.4826 - val_accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "# Reset the RNG state\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build the same model but with PReLU activation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model for 10 epochs\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-think",
   "metadata": {},
   "source": [
    "#### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "velvet-belle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "# Plot the ELU activation function\n",
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-expense",
   "metadata": {},
   "source": [
    "#### SELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ranging-legislature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)\n",
    "\n",
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)\n",
    "\n",
    "# Plot the SELU activation function\n",
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "academic-picture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "# Reset the numpy RNG\n",
    "np.random.seed(42)\n",
    "\n",
    "# Standardized inputs\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "\n",
    "# Stack of 1k dense layers\n",
    "for layer in range(1000):\n",
    "    # LeCun initialization\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "utility-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f81e5e46fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "generic-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 27s 14ms/step - loss: 1.3414 - accuracy: 0.4814 - val_loss: 0.7377 - val_accuracy: 0.7502\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 24s 14ms/step - loss: 0.7292 - accuracy: 0.7406 - val_loss: 0.5861 - val_accuracy: 0.7832\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.5890 - accuracy: 0.7893 - val_loss: 0.6003 - val_accuracy: 0.7920\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.5440 - accuracy: 0.8130 - val_loss: 0.4992 - val_accuracy: 0.8310\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.5055 - accuracy: 0.8239 - val_loss: 0.4923 - val_accuracy: 0.8300\n"
     ]
    }
   ],
   "source": [
    "# Reset the RNG state\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build a 100 layer DNN with SELU activation and LeCun initialization\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Scale the Fashion MNIST inputs to mean 0 and std 1\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "potential-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 25s 13ms/step - loss: 2.0605 - accuracy: 0.1875 - val_loss: 1.2408 - val_accuracy: 0.4554\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 1.2370 - accuracy: 0.4725 - val_loss: 0.9193 - val_accuracy: 0.5934\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.9940 - accuracy: 0.5775 - val_loss: 1.1140 - val_accuracy: 0.5152\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.9765 - accuracy: 0.5971 - val_loss: 0.7640 - val_accuracy: 0.7128\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.7494 - accuracy: 0.7122 - val_loss: 0.7422 - val_accuracy: 0.7186\n"
     ]
    }
   ],
   "source": [
    "# Reset the RNG state\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Build the same model but with ReLU activation for comparison\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-russian",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "The problem that *batch normalization* addresses is to prevent gradient from vanishing/exploding even during training, i.e. not just initialization. The idea is to keep layer inputs zero-centered and normalized by adding an extra operation before or after activation function. The algorithm is following:\n",
    "1. $\\mathbf{\\mu}_B = \\frac{1}{m_B}\\sum_{i = 1}^{m_B} \\mathbf{x}^{(i)}$ .................... $\\mathbf{\\mu}_B$ is the a of input means evaluated over the whole mini-batch $B$ containing $m_B$ instances\n",
    "1. $\\mathbf{\\sigma}_B^2 = \\frac{1}{m_B}\\sum_{i = 1}^{m_B} (\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B)^2$ ..... $\\mathbf{\\sigma}_B$ is a vector of input standard deviations\n",
    "1. $\\hat{\\mathbf{x}}^{(i)} = \\frac{\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B}{\\sqrt{\\mathbf{\\sigma}_B^2 + \\epsilon}}$ ............................... $\\hat{\\mathbf{x}}^{(i)}$ is a vector of zero-centered and normalized inputs for instance $i$ where $\\epsilon$ is a *smootning term* (a small number preventing division by zero)\n",
    "1. $\\mathbf{z}^{(i)} = \\mathbf{\\gamma} \\otimes \\hat{\\mathbf{x}}^{(i)} + \\mathbf{\\beta}$ ...................... $\\mathbf{z}^{(i)}$ is the output of BN operation, it is a rescaled and shifted version of the inputs where $\\mathbf{\\gamma}$ ($\\mathbf{\\beta}$) is a scaling (shift/offset) parameter of the layer and $\\otimes$ is element-wise multiplication\n",
    "\n",
    "Wheat remains is to answer how to normalize test instances which are applied one after another (there are no batches, moreover test instances may not be *iid*, so computing any reliable statistic is not possible). In other words what is the final $\\mathbf{\\mu}_B$ and $\\mathbf{\\sigma}_B^2$ ($\\mathbf{\\gamma}$ and $\\mathbf{\\beta}$ are learned during trainging as additional parameters)? There are two options - the latter is typically implemented (e.g. by Keras):\n",
    "1. Run the whole training set through the network again after training to compute the instance means and std\n",
    "1. Estimate these during traingin as exponential moving averages: $\\hat{\\mathbf{v}} \\gets \\text{momentum} \\times \\hat{\\mathbf{v}} + (1 - \\text{momentum}) \\times \\mathbf{v}$ where *momentum* is a BN hyper-parameter, $\\hat{\\mathbf{v}}$ is the exp. moving avg. and $\\mathbf{v}$ are current batch means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confused-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a sequential model with batch normalization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Show model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "latest-alpha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: True\tName: batch_normalization/gamma:0\n",
      "Trainable: True\tName: batch_normalization/beta:0\n",
      "Trainable: False\tName: batch_normalization/moving_mean:0\n",
      "Trainable: False\tName: batch_normalization/moving_variance:0\n"
     ]
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "for var in bn1.variables:\n",
    "    print(f\"Trainable: {var.trainable}\\tName: {var.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beneficial-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 1.2287 - accuracy: 0.5993 - val_loss: 0.5526 - val_accuracy: 0.8230\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5996 - accuracy: 0.7960 - val_loss: 0.4726 - val_accuracy: 0.8470\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5312 - accuracy: 0.8170 - val_loss: 0.4376 - val_accuracy: 0.8546\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4884 - accuracy: 0.8293 - val_loss: 0.4153 - val_accuracy: 0.8600\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4718 - accuracy: 0.8344 - val_loss: 0.3997 - val_accuracy: 0.8644\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4419 - accuracy: 0.8459 - val_loss: 0.3867 - val_accuracy: 0.8692\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4286 - accuracy: 0.8496 - val_loss: 0.3764 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4086 - accuracy: 0.8553 - val_loss: 0.3713 - val_accuracy: 0.8736\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4079 - accuracy: 0.8566 - val_loss: 0.3631 - val_accuracy: 0.8756\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3903 - accuracy: 0.8616 - val_loss: 0.3573 - val_accuracy: 0.8756\n"
     ]
    }
   ],
   "source": [
    "# Compile and train the model for 10 epochs\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-thickness",
   "metadata": {},
   "source": [
    "Sometimes applying BN before the activation function works better. One advantage of doing so is that the batch-normalized layer does not need a bias term and we can save some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lasting-victorian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 1.3677 - accuracy: 0.5605 - val_loss: 0.6767 - val_accuracy: 0.7812\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.7135 - accuracy: 0.7703 - val_loss: 0.5566 - val_accuracy: 0.8186\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6123 - accuracy: 0.7991 - val_loss: 0.5007 - val_accuracy: 0.8364\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5547 - accuracy: 0.8148 - val_loss: 0.4666 - val_accuracy: 0.8450\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5254 - accuracy: 0.8232 - val_loss: 0.4434 - val_accuracy: 0.8536\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4947 - accuracy: 0.8325 - val_loss: 0.4263 - val_accuracy: 0.8548\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4736 - accuracy: 0.8387 - val_loss: 0.4131 - val_accuracy: 0.8570\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4550 - accuracy: 0.8444 - val_loss: 0.4035 - val_accuracy: 0.8612\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4495 - accuracy: 0.8438 - val_loss: 0.3943 - val_accuracy: 0.8642\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4333 - accuracy: 0.8494 - val_loss: 0.3875 - val_accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "# Build and train a batch-normalized model\n",
    "#  - normalization is used before activation and biases are switched off\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-topic",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "This technique is most often used for RNNs where using BN is quite tricky. Gradient clipping can be set for an optimizer by one of two parameters:\n",
    "* `clipvalue` - simply clips values to fit in given bounds\n",
    "* `clipnorm` - preserves gradient direction by using $\\ell_2$ norm instead (but while doing this it can produce values close to 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-meeting",
   "metadata": {},
   "source": [
    "Following will transform gradient values $[0.9, 100.0] \\to [0.9, 1.0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "partial-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-screw",
   "metadata": {},
   "source": [
    "While using the norm will produce $[0.9, 100.0] \\to [0.00899964, 0.9999595]$ (notice that first component is very close to zero so this option does not really help with the vanishing gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fifth-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-default",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers\n",
    "\n",
    "#### Reusing a Keras model\n",
    "Let's take a Fashion MNIST dataset and pretend there's an existing pre-trained model that can classify images to all but two classes. Out task is to classify images to these two classes (sandals and shirts).\n",
    "\n",
    "This example is fine-tuned in the stype of *torturing the data until it confesses*, typically *transfer learning* does not work for shallow and wide nets because the patterns learned in lower layers are not general enough. On the contrary, it works pretty well for deep convolutional networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "synthetic-score",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.9249 - accuracy: 0.6994 - val_loss: 0.3896 - val_accuracy: 0.8662\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.3651 - accuracy: 0.8745 - val_loss: 0.3286 - val_accuracy: 0.8832\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.3182 - accuracy: 0.8894 - val_loss: 0.3014 - val_accuracy: 0.8986\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.3048 - accuracy: 0.8956 - val_loss: 0.2896 - val_accuracy: 0.9016\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 3s 3ms/step - loss: 0.2804 - accuracy: 0.9029 - val_loss: 0.2777 - val_accuracy: 0.9066\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2701 - accuracy: 0.9077 - val_loss: 0.2735 - val_accuracy: 0.9068\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2627 - accuracy: 0.9094 - val_loss: 0.2716 - val_accuracy: 0.9086\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2610 - accuracy: 0.9118 - val_loss: 0.2590 - val_accuracy: 0.9141\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 3s 2ms/step - loss: 0.2558 - accuracy: 0.9110 - val_loss: 0.2561 - val_accuracy: 0.9138\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2512 - accuracy: 0.9135 - val_loss: 0.2542 - val_accuracy: 0.9163\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2431 - accuracy: 0.9170 - val_loss: 0.2497 - val_accuracy: 0.9150\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2422 - accuracy: 0.9168 - val_loss: 0.2509 - val_accuracy: 0.9128\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2360 - accuracy: 0.9182 - val_loss: 0.2446 - val_accuracy: 0.9158\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2267 - accuracy: 0.9231 - val_loss: 0.2415 - val_accuracy: 0.9175\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 5s 3ms/step - loss: 0.2225 - accuracy: 0.9239 - val_loss: 0.2446 - val_accuracy: 0.9193\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2262 - accuracy: 0.9214 - val_loss: 0.2385 - val_accuracy: 0.9198\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 4s 3ms/step - loss: 0.2192 - accuracy: 0.9250 - val_loss: 0.2409 - val_accuracy: 0.9175\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 5s 3ms/step - loss: 0.2171 - accuracy: 0.9255 - val_loss: 0.2429 - val_accuracy: 0.9153\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2181 - accuracy: 0.9247 - val_loss: 0.2332 - val_accuracy: 0.9193\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2113 - accuracy: 0.9270 - val_loss: 0.2333 - val_accuracy: 0.9203\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Reset RNG state\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split the dataset into\n",
    "# A - data that our pre-trained model is trained on (not containing sandals and shirts)\n",
    "# B - small dataset for our taks (containing sandals and shirts)\n",
    "\n",
    "sandals_class, shirts_class = 5, 6\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    # sandals or shirts\n",
    "    y_sandals_or_shirts = (y == sandals_class) | (y == shirts_class)\n",
    "    y_A = y[~y_sandals_or_shirts]\n",
    "    # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_A[y_A > shirts_class] -= 2\n",
    "    # binary classification task: is it a shirt (class 6)?\n",
    "    y_B = (y[y_sandals_or_shirts] == shirts_class).astype(np.float32)\n",
    "    return (X[~y_sandals_or_shirts], y_A), (X[y_sandals_or_shirts], y_B)\n",
    "\n",
    "# Split the data into training, validation and test sets for tasks A and B\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "\n",
    "# Sub-select a small dataset for taks B (sandals and shirts)\n",
    "dataset_size = 200\n",
    "X_train_B = X_train_B[:dataset_size]\n",
    "y_train_B = y_train_B[:dataset_size]\n",
    "\n",
    "\n",
    "# Build and pre-train and save model A\n",
    "model_A_path = os.path.join(\"data\", \"my_model_A.h5\")\n",
    "\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n",
    "\n",
    "model_A.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20, validation_data=(X_valid_A, y_valid_A))\n",
    "model_A.save(model_A_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accomplished-substitute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 62ms/step - loss: 1.0360 - accuracy: 0.4975 - val_loss: 0.6314 - val_accuracy: 0.6004\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.5883 - accuracy: 0.6971 - val_loss: 0.4784 - val_accuracy: 0.8529\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.4380 - accuracy: 0.8854 - val_loss: 0.4102 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.4021 - accuracy: 0.8712 - val_loss: 0.3647 - val_accuracy: 0.9178\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3361 - accuracy: 0.9348 - val_loss: 0.3300 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.3113 - accuracy: 0.9233 - val_loss: 0.3019 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2817 - accuracy: 0.9299 - val_loss: 0.2804 - val_accuracy: 0.9422\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2632 - accuracy: 0.9379 - val_loss: 0.2606 - val_accuracy: 0.9473\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.2373 - accuracy: 0.9481 - val_loss: 0.2428 - val_accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2229 - accuracy: 0.9657 - val_loss: 0.2281 - val_accuracy: 0.9544\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.2155 - accuracy: 0.9590 - val_loss: 0.2150 - val_accuracy: 0.9584\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1834 - accuracy: 0.9738 - val_loss: 0.2036 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1671 - accuracy: 0.9828 - val_loss: 0.1931 - val_accuracy: 0.9615\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1527 - accuracy: 0.9915 - val_loss: 0.1838 - val_accuracy: 0.9635\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1595 - accuracy: 0.9904 - val_loss: 0.1746 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1473 - accuracy: 0.9937 - val_loss: 0.1674 - val_accuracy: 0.9686\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1412 - accuracy: 0.9944 - val_loss: 0.1604 - val_accuracy: 0.9706\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1242 - accuracy: 0.9931 - val_loss: 0.1539 - val_accuracy: 0.9706\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.1224 - accuracy: 0.9931 - val_loss: 0.1482 - val_accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1096 - accuracy: 0.9912 - val_loss: 0.1431 - val_accuracy: 0.9716\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1408 - accuracy: 0.9705\n"
     ]
    }
   ],
   "source": [
    "# Next let's use the same architecture for taks B\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_B.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train this model on just 200 instances for task B\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20, validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "# Evaluate model B on the test set\n",
    "_, acc_B = model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-graphic",
   "metadata": {},
   "source": [
    "Next let's try to reuse model A for task B. One can either directly link model A's layers and weights (`model.layers[:-1]`) to model B or clone model A (and it's weights) to make an independent model B like so\n",
    "```python\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "pressed-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 70ms/step - loss: 0.6135 - accuracy: 0.6184 - val_loss: 0.5822 - val_accuracy: 0.6359\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5530 - accuracy: 0.6638 - val_loss: 0.5449 - val_accuracy: 0.6805\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4874 - accuracy: 0.7531 - val_loss: 0.5129 - val_accuracy: 0.7099\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.4878 - accuracy: 0.7405 - val_loss: 0.4843 - val_accuracy: 0.7333\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 69ms/step - loss: 0.4367 - accuracy: 0.7823 - val_loss: 0.3452 - val_accuracy: 0.8641\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.2963 - accuracy: 0.9143 - val_loss: 0.2599 - val_accuracy: 0.9300\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.2029 - accuracy: 0.9777 - val_loss: 0.2108 - val_accuracy: 0.9554\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1749 - accuracy: 0.9789 - val_loss: 0.1789 - val_accuracy: 0.9696\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1344 - accuracy: 0.9809 - val_loss: 0.1561 - val_accuracy: 0.9757\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1171 - accuracy: 0.9973 - val_loss: 0.1393 - val_accuracy: 0.9797\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1135 - accuracy: 0.9931 - val_loss: 0.1267 - val_accuracy: 0.9838\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0999 - accuracy: 0.9931 - val_loss: 0.1164 - val_accuracy: 0.9858\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.0834 - accuracy: 1.0000 - val_loss: 0.1067 - val_accuracy: 0.9888\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0776 - accuracy: 1.0000 - val_loss: 0.1001 - val_accuracy: 0.9899\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0690 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 0.9899\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0718 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9899\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.0840 - val_accuracy: 0.9899\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0494 - accuracy: 1.0000 - val_loss: 0.0803 - val_accuracy: 0.9899\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0544 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9899\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0472 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9899\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0683 - accuracy: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0002272564418244"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transfer_learn(pretrained_model, X_train, X_valid, y_train, y_valid, epochs_init=4, epochs_train=16, lr_init=1e-3, lr_train=1e-3):\n",
    "    # Reuse all but output layers and and new output for this task\n",
    "    reused_layers = pretrained_model.layers[:-1]\n",
    "    model = keras.models.Sequential(reused_layers)\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # Freeze weights of reused layers' weights for first couple of epochs\n",
    "    for layer in reused_layers:\n",
    "        layer.trainable = False\n",
    "     \n",
    "    # Model must be re-compiled each time we modify layers\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.SGD(lr=lr_init),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    # Short initial training with weights reused layers being locked to learn reasonable weights for the output\n",
    "    history = model.fit(X_train, y_train, epochs=epochs_init, validation_data=(X_valid, y_valid))\n",
    "    \n",
    "    # Unlock all weights\n",
    "    for layer in reused_layers:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    # Recompile the model - use small learning rate so the reused weights are not completely destroyed\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=keras.optimizers.SGD(lr=lr_train),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs_train, validation_data=(X_valid, y_valid))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load pre-trained model for task A\n",
    "model_A = keras.models.load_model(model_A_path)\n",
    "\n",
    "# Transfer-learn model for task B\n",
    "model_B = transfer_learn(model_A, X_train_B, X_valid_B, y_train_B, y_valid_B)\n",
    "\n",
    "# Evaluate new model B on the test set\n",
    "_, acc_B_tl = model_B.evaluate(X_test_B, y_test_B)\n",
    "\n",
    "# Display error rate imrovement\n",
    "acc_improvement = (100 - acc_B) / (100 - acc_B_tl)\n",
    "acc_improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-tourism",
   "metadata": {},
   "source": [
    "## Faster Optimizers\n",
    "Standard gradient discent step: $\\mathbf{\\theta} \\gets \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "\n",
    "### Momentum optimization\n",
    "The idea is to use the gradient not as a speed but as an acceleration (just like a ball rolling down picks up speed). The update in this case is two fold:\n",
    "1. $\\mathbf{m} \\gets \\beta \\mathbf{m} - \\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "1. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} + \\mathbf{m}$\n",
    "\n",
    "where $\\mathbf{m}$ is the momentum vector and $\\beta$ is new hyper-parameter called *momentum* which serges as a friction mechanism and should take value from $[0, 1]$ (0 being high friction, 1 for no friction).\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "```\n",
    "\n",
    "### Nesterov Accelerated Gradient\n",
    "This technique is generally an improved variant to vanilla momentum optimization which does not compute the gradient at current location $\\mathbf{\\theta}$ but a small step $\\mathbf{\\theta} + \\beta \\mathbf{m}$ ahead: \n",
    "1. $\\mathbf{m} \\gets \\beta \\mathbf{m} - \\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta} + \\beta \\mathbf{m})$\n",
    "1. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} + \\mathbf{m}$\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "```\n",
    "\n",
    "### AdaGrad\n",
    "The idea behind *AdaGrad* is to point the gradient more towards the optimum by scaling down the gradient vector by factor of $\\sqrt{\\mathbf{s} + \\epsilon}$ where $s$ are accumulated squeres of gradients.\n",
    "1. $\\mathbf{s} \\gets \\mathbf{s} + \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "1. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\oslash \\sqrt{\\mathbf{s} + \\epsilon}$\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.Adagrad(lr=0.001)\n",
    "```\n",
    "\n",
    "The issue of *AdaGrad* is that effectively this algorithm decays the learning rate as $\\frac{\\eta}{\\sqrt{s_i + \\epsilon}}$ but this $s_i$ keeps accumulating and so the whole term goes to 0. This means that the learning and whole algorithm stops prematurely!\n",
    "\n",
    "### RMSProp\n",
    "*RMSProp* fixes the issue of *AdaGrad* by accumulating only the gradients from recent iterations. It does so by using exponential decay:\n",
    "1. $\\mathbf{s} \\gets \\beta \\mathbf{s} + (1 - \\beta) \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "1. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} - \\eta \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\oslash \\sqrt{\\mathbf{s} + \\epsilon}$\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "```\n",
    "\n",
    "### Adam Optimization\n",
    "*Adaptive moment estimation* or *Adam* combines the idea of moment optimization and *RMSProp*:\n",
    "* like momentum opt. it keeps track of exponentially decaying average of past gradients\n",
    "* like *RMSProp* it keeps track of exponentially decaying average of past squared gradients\n",
    "\n",
    "1. $\\mathbf{m} \\gets \\beta_1 \\mathbf{m} - (1 - \\beta_1) \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "1. $\\mathbf{s} \\gets \\beta_2 \\mathbf{s} + (1 - \\beta_2) \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\otimes \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})$\n",
    "1. $\\hat{\\mathbf{m}} \\gets \\frac{\\mathbf{m}}{1 - \\beta_1^t}$\n",
    "1. $\\hat{\\mathbf{s}} \\gets \\frac{\\mathbf{s}}{1 - \\beta_w^t}$\n",
    "1. $\\mathbf{\\theta} \\gets \\mathbf{\\theta} + \\eta \\; \\hat{\\mathbf{m}} \\oslash \\sqrt{\\hat{\\mathbf{s}} + \\epsilon}$\n",
    "\n",
    "Steps 1, 2 and 5 resemble momentum opt. and RMSProp (step 1 just uses exp. decaying average rather than sum which is equivalent except a constant). Steps 2 and 3 are a technical detail - since we initialize $\\mathbf{m}$ and $\\mathbf{s}$ to 0, these would be biased towards it at the begining of the training - these steps help with it at the start.\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "\n",
    "### Adamax Optimization\n",
    "*Adamax* is a variation of *Adam* that replaces step 2 with \n",
    "\n",
    "$\\mathbf{s} \\gets \\max{\\beta_2 \\mathbf{s}, \\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}}$\n",
    "\n",
    "The idea here is that in Adam we first accumulate (sum) the squares of gradients and then scale the learning rate down by a square root ($\\epsilon$ is just a technicality) which basically means that Adam scales the parameter updates down by the $\\ell_2$ norm of the time-decayed gradients. Adamax simply replaces this by an $\\ell_\\infty$ norm, also known as the max-norm.\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```\n",
    "\n",
    "### Nadam Optimization\n",
    "Finally, *Nadam* just adds the Nesterov trick to Adam which typically improves the convergence speed.\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-bibliography",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-party",
   "metadata": {},
   "source": [
    "### Power Scheduling\n",
    "\n",
    "*Power Scheduling* gradually decays the learning rate over the course of learning. Learning rate update is $\\text{lr} \\gets \\text{lr}_0 * (1 + \\frac{\\text{steps}}{s})^{-c}$. Note that Keras uses $c = 1$ and $s = \\frac{1}{\\text{decay}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "neutral-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6048 - accuracy: 0.7907 - val_loss: 0.4050 - val_accuracy: 0.8576\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3864 - accuracy: 0.8635 - val_loss: 0.3739 - val_accuracy: 0.8666\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3500 - accuracy: 0.8792 - val_loss: 0.3718 - val_accuracy: 0.8696\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3290 - accuracy: 0.8828 - val_loss: 0.3505 - val_accuracy: 0.8760\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3174 - accuracy: 0.8877 - val_loss: 0.3439 - val_accuracy: 0.8782\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2921 - accuracy: 0.8954 - val_loss: 0.3428 - val_accuracy: 0.8768\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2865 - accuracy: 0.8979 - val_loss: 0.3371 - val_accuracy: 0.8802\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2720 - accuracy: 0.9024 - val_loss: 0.3418 - val_accuracy: 0.8748\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2708 - accuracy: 0.9029 - val_loss: 0.3292 - val_accuracy: 0.8812\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2572 - accuracy: 0.9084 - val_loss: 0.3283 - val_accuracy: 0.8842\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2514 - accuracy: 0.9110 - val_loss: 0.3292 - val_accuracy: 0.8852\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2457 - accuracy: 0.9135 - val_loss: 0.3335 - val_accuracy: 0.8796\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2400 - accuracy: 0.9157 - val_loss: 0.3257 - val_accuracy: 0.8838\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2361 - accuracy: 0.9159 - val_loss: 0.3287 - val_accuracy: 0.8866\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2351 - accuracy: 0.9176 - val_loss: 0.3239 - val_accuracy: 0.8854\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2287 - accuracy: 0.9193 - val_loss: 0.3230 - val_accuracy: 0.8868\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2203 - accuracy: 0.9219 - val_loss: 0.3256 - val_accuracy: 0.8882\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2221 - accuracy: 0.9219 - val_loss: 0.3203 - val_accuracy: 0.8888\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2217 - accuracy: 0.9223 - val_loss: 0.3240 - val_accuracy: 0.8890\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2192 - accuracy: 0.9236 - val_loss: 0.3210 - val_accuracy: 0.8866\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2172 - accuracy: 0.9248 - val_loss: 0.3229 - val_accuracy: 0.8856\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2124 - accuracy: 0.9246 - val_loss: 0.3190 - val_accuracy: 0.8886\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2103 - accuracy: 0.9262 - val_loss: 0.3214 - val_accuracy: 0.8848\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2041 - accuracy: 0.9301 - val_loss: 0.3221 - val_accuracy: 0.8876\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2064 - accuracy: 0.9289 - val_loss: 0.3209 - val_accuracy: 0.8886\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "# SGD with Power Scheduling\n",
    "optimizer = keras.optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "\n",
    "# Build an example model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 25 epochs\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# Replicate the learning rate schedule\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "# Plot the learning rate schedule\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-question",
   "metadata": {},
   "source": [
    "### Exponential Scheduling\n",
    "While the decay in *power scheduling* gets smaller over the course of the learning process, *exponential scheduling* keeps the same rate of decay and updates the learning rate as $\\text{lr} \\gets \\text{lr}_0 * 0.1^{\\frac{\\text{epoch}}{s}}$. The interpretation is that the learning rate drops 10x each $s$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "running-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 1.1143 - accuracy: 0.7297 - val_loss: 0.9693 - val_accuracy: 0.7078\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.7630 - accuracy: 0.7744 - val_loss: 0.7304 - val_accuracy: 0.8070\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.6632 - accuracy: 0.8110 - val_loss: 0.6339 - val_accuracy: 0.8208\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.5770 - accuracy: 0.8315 - val_loss: 0.6525 - val_accuracy: 0.8222\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.5520 - accuracy: 0.8381 - val_loss: 0.4839 - val_accuracy: 0.8612\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.4763 - accuracy: 0.8622 - val_loss: 0.5255 - val_accuracy: 0.8624\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4379 - accuracy: 0.8694 - val_loss: 0.5140 - val_accuracy: 0.8610\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4037 - accuracy: 0.8765 - val_loss: 0.5957 - val_accuracy: 0.8594\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3875 - accuracy: 0.8799 - val_loss: 0.5060 - val_accuracy: 0.8668\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3480 - accuracy: 0.8933 - val_loss: 0.4657 - val_accuracy: 0.8762\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3250 - accuracy: 0.8978 - val_loss: 0.4756 - val_accuracy: 0.8830\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2902 - accuracy: 0.9047 - val_loss: 0.4718 - val_accuracy: 0.8676\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2691 - accuracy: 0.9103 - val_loss: 0.4623 - val_accuracy: 0.8868\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2500 - accuracy: 0.9185 - val_loss: 0.4706 - val_accuracy: 0.8836\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2388 - accuracy: 0.9226 - val_loss: 0.4431 - val_accuracy: 0.8880\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2183 - accuracy: 0.9258 - val_loss: 0.4606 - val_accuracy: 0.8884\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2009 - accuracy: 0.9333 - val_loss: 0.4784 - val_accuracy: 0.8844\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.1907 - accuracy: 0.9365 - val_loss: 0.4658 - val_accuracy: 0.8880\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1792 - accuracy: 0.9412 - val_loss: 0.5151 - val_accuracy: 0.8966\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1655 - accuracy: 0.9435 - val_loss: 0.4909 - val_accuracy: 0.8908\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1575 - accuracy: 0.9477 - val_loss: 0.5190 - val_accuracy: 0.8884\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1430 - accuracy: 0.9525 - val_loss: 0.5549 - val_accuracy: 0.8934\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.1371 - accuracy: 0.9553 - val_loss: 0.5340 - val_accuracy: 0.8884\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1238 - accuracy: 0.9608 - val_loss: 0.5835 - val_accuracy: 0.8886\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1185 - accuracy: 0.9622 - val_loss: 0.5782 - val_accuracy: 0.8920\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial learning rate can be ither hard-coded\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)\n",
    "\n",
    "# Or one can write a factory function for the schedule function\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "# This is equivalent to the function defined earlier\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "# Build the same model as before\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 25 epochs with a scheduler callback using our learning rate function\n",
    "n_epochs = 25\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_scheduler])\n",
    "\n",
    "# Plot the learning rate schedule\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-today",
   "metadata": {},
   "source": [
    "The schedule function can take the current learning rate as a second argument:\n",
    "```python\n",
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-taste",
   "metadata": {},
   "source": [
    "If you want to update the learning rate at each iteration rather than at each epoch, you must write your own callback class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "million-complexity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 17s 9ms/step - loss: 1.0964 - accuracy: 0.7381 - val_loss: 0.8591 - val_accuracy: 0.7354\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.6380 - accuracy: 0.8002 - val_loss: 0.5800 - val_accuracy: 0.8178\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.6463 - accuracy: 0.8061 - val_loss: 0.7903 - val_accuracy: 0.7388\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5711 - accuracy: 0.8257 - val_loss: 0.6030 - val_accuracy: 0.8280\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.5411 - accuracy: 0.8364 - val_loss: 0.5031 - val_accuracy: 0.8468\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.4668 - accuracy: 0.8561 - val_loss: 0.4598 - val_accuracy: 0.8606\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.4344 - accuracy: 0.8674 - val_loss: 0.4830 - val_accuracy: 0.8564\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.3959 - accuracy: 0.8747 - val_loss: 0.5399 - val_accuracy: 0.8440\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.3870 - accuracy: 0.8782 - val_loss: 0.5439 - val_accuracy: 0.8602\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.3531 - accuracy: 0.8878 - val_loss: 0.4852 - val_accuracy: 0.8698\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.3201 - accuracy: 0.8979 - val_loss: 0.4318 - val_accuracy: 0.8738\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3107 - accuracy: 0.8990 - val_loss: 0.4609 - val_accuracy: 0.8700\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2942 - accuracy: 0.9064 - val_loss: 0.4662 - val_accuracy: 0.8738\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2728 - accuracy: 0.9114 - val_loss: 0.4584 - val_accuracy: 0.8690\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2590 - accuracy: 0.9163 - val_loss: 0.4799 - val_accuracy: 0.8782\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2430 - accuracy: 0.9208 - val_loss: 0.4878 - val_accuracy: 0.8828\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.2214 - accuracy: 0.9293 - val_loss: 0.4731 - val_accuracy: 0.8810\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2111 - accuracy: 0.9325 - val_loss: 0.4950 - val_accuracy: 0.8778\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.2016 - accuracy: 0.9369 - val_loss: 0.4779 - val_accuracy: 0.8878\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.1953 - accuracy: 0.9387 - val_loss: 0.4624 - val_accuracy: 0.8812\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.1757 - accuracy: 0.9422 - val_loss: 0.4622 - val_accuracy: 0.8856\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.1586 - accuracy: 0.9432 - val_loss: 0.4653 - val_accuracy: 0.8854\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.1472 - accuracy: 0.9464 - val_loss: 0.5050 - val_accuracy: 0.8838\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.1364 - accuracy: 0.9500 - val_loss: 0.5185 - val_accuracy: 0.8838\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.1328 - accuracy: 0.9496 - val_loss: 0.5222 - val_accuracy: 0.8820\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Callback that applies exponential decay each SGD iteration\n",
    "#  - with previous schedules the learning rate was adapted per epoch\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: the `batch` argument is reset at each epoch\n",
    "        lr = keras.backend.get_value(self.model.optimizer.lr)\n",
    "        keras.backend.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = keras.backend.get_value(self.model.optimizer.lr)\n",
    "\n",
    "# Define the SGD optimizer with initial learning rate\n",
    "lr0 = 0.01\n",
    "optimizer = keras.optimizers.Nadam(lr=lr0)\n",
    "        \n",
    "# Build the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 25 epochs passing in the exponential decay callback\n",
    "n_epochs = 25\n",
    "batch_size = 32\n",
    "s = 20 * len(X_train) // batch_size # number of steps in 20 epochs (batch size = 32)\n",
    "exp_decay = ExponentialDecay(s)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[exp_decay])\n",
    "\n",
    "# Replicate the learning rate schedule\n",
    "n_steps = n_epochs * len(X_train) // batch_size\n",
    "steps = np.arange(n_steps)\n",
    "lrs = lr0 * 0.1**(steps / s)\n",
    "\n",
    "# Plot the learning rate schedule\n",
    "plt.plot(steps, lrs, \"-\", linewidth=2)\n",
    "plt.axis([0, n_steps - 1, 0, lr0 * 1.1])\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling (per batch)\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-capacity",
   "metadata": {},
   "source": [
    "### Piecewise Constant Scheduling\n",
    "As the name suggests, *piecewise constant scheduling* simply defines multiple but fixed learning rates which it applies in order based on current step interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "junior-things",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 1.1173 - accuracy: 0.7325 - val_loss: 0.8061 - val_accuracy: 0.7438\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.7935 - accuracy: 0.7690 - val_loss: 0.8565 - val_accuracy: 0.7978\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.9195 - accuracy: 0.7502 - val_loss: 0.9441 - val_accuracy: 0.6892\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.9097 - accuracy: 0.7416 - val_loss: 0.6830 - val_accuracy: 0.8026\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.8189 - accuracy: 0.7520 - val_loss: 1.3275 - val_accuracy: 0.7242\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.6835 - accuracy: 0.8034 - val_loss: 0.6031 - val_accuracy: 0.8274\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.5304 - accuracy: 0.8403 - val_loss: 0.6102 - val_accuracy: 0.8318\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.5046 - accuracy: 0.8413 - val_loss: 0.6648 - val_accuracy: 0.8194\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4973 - accuracy: 0.8467 - val_loss: 0.5875 - val_accuracy: 0.8544\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 15s 8ms/step - loss: 0.4721 - accuracy: 0.8529 - val_loss: 0.6824 - val_accuracy: 0.8464\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4722 - accuracy: 0.8564 - val_loss: 0.6252 - val_accuracy: 0.8368\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4693 - accuracy: 0.8564 - val_loss: 0.6424 - val_accuracy: 0.8420\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4535 - accuracy: 0.8653 - val_loss: 0.6592 - val_accuracy: 0.8374\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4510 - accuracy: 0.8641 - val_loss: 0.6160 - val_accuracy: 0.8386\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4535 - accuracy: 0.8644 - val_loss: 0.7624 - val_accuracy: 0.8558\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3497 - accuracy: 0.8872 - val_loss: 0.5350 - val_accuracy: 0.8674\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2955 - accuracy: 0.9022 - val_loss: 0.5309 - val_accuracy: 0.8706\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2925 - accuracy: 0.9053 - val_loss: 0.4934 - val_accuracy: 0.8708\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2814 - accuracy: 0.9080 - val_loss: 0.5428 - val_accuracy: 0.8712\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2766 - accuracy: 0.9098 - val_loss: 0.5367 - val_accuracy: 0.8744\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2737 - accuracy: 0.9101 - val_loss: 0.5370 - val_accuracy: 0.8716\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2611 - accuracy: 0.9152 - val_loss: 0.5869 - val_accuracy: 0.8764\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2569 - accuracy: 0.9163 - val_loss: 0.5838 - val_accuracy: 0.8702\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2473 - accuracy: 0.9195 - val_loss: 0.6104 - val_accuracy: 0.8728\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2574 - accuracy: 0.9169 - val_loss: 0.5948 - val_accuracy: 0.8780\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple version of a piecewise constant schedule\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "\n",
    "# More general version of piecewise constant schedule\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "# This call creates schedule that is equivalent to the one defined earlier\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
    "\n",
    "# Make a scheduler callback using the piecewise constant schedule\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "# Build a model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 25 epochs using the piecewise constant schedule\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_scheduler])\n",
    "\n",
    "# Plot the piecewise constant schedule\n",
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-bacon",
   "metadata": {},
   "source": [
    "### Performance Scheduling\n",
    "*Performance Scheduling* works similarly to *early stopping* strategy - it measures the validation error some number of steps and if it does not increase for certain number of steps the learning rate is adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "strange-marathon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.7294 - accuracy: 0.7758 - val_loss: 0.4786 - val_accuracy: 0.8490\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4837 - accuracy: 0.8383 - val_loss: 0.6440 - val_accuracy: 0.8268\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5055 - accuracy: 0.8415 - val_loss: 0.5156 - val_accuracy: 0.8496\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4992 - accuracy: 0.8463 - val_loss: 0.5767 - val_accuracy: 0.8456\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5012 - accuracy: 0.8493 - val_loss: 0.4996 - val_accuracy: 0.8536\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4848 - accuracy: 0.8571 - val_loss: 0.5720 - val_accuracy: 0.8550\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3359 - accuracy: 0.8874 - val_loss: 0.3825 - val_accuracy: 0.8766\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2514 - accuracy: 0.9076 - val_loss: 0.3668 - val_accuracy: 0.8804\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2341 - accuracy: 0.9121 - val_loss: 0.3732 - val_accuracy: 0.8812\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2163 - accuracy: 0.9205 - val_loss: 0.3943 - val_accuracy: 0.8920\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1942 - accuracy: 0.9277 - val_loss: 0.3955 - val_accuracy: 0.8876\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1882 - accuracy: 0.9287 - val_loss: 0.4801 - val_accuracy: 0.8736\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1809 - accuracy: 0.9330 - val_loss: 0.4397 - val_accuracy: 0.8876\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1317 - accuracy: 0.9475 - val_loss: 0.4328 - val_accuracy: 0.8926\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.1153 - accuracy: 0.9562 - val_loss: 0.4101 - val_accuracy: 0.8916\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1027 - accuracy: 0.9609 - val_loss: 0.4328 - val_accuracy: 0.8932\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0927 - accuracy: 0.9643 - val_loss: 0.4591 - val_accuracy: 0.8910\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0861 - accuracy: 0.9676 - val_loss: 0.4639 - val_accuracy: 0.8926\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0731 - accuracy: 0.9733 - val_loss: 0.4753 - val_accuracy: 0.8980\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0630 - accuracy: 0.9768 - val_loss: 0.4853 - val_accuracy: 0.8932\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0584 - accuracy: 0.9791 - val_loss: 0.4873 - val_accuracy: 0.8938\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0576 - accuracy: 0.9795 - val_loss: 0.5070 - val_accuracy: 0.8974\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0542 - accuracy: 0.9808 - val_loss: 0.5167 - val_accuracy: 0.8958\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0447 - accuracy: 0.9857 - val_loss: 0.5187 - val_accuracy: 0.8972\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.0417 - accuracy: 0.9869 - val_loss: 0.5300 - val_accuracy: 0.8932\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a schedule callback that updates learning rate by given factor if a validation error hits a plateau for given no. steps\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "# Define an optimizer with momentum and initial learning rate\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)\n",
    "\n",
    "# Build a model using the optimizer\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 25 epochs using the performance scheduler\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid), callbacks=[lr_scheduler])\n",
    "\n",
    "# Plot the learning rate history\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color=\"b\")\n",
    "plt.tick_params(\"y\", colors=\"b\")\n",
    "plt.gca().set_xlim(0, n_epochs - 1)\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the validation error to the same figure\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color=\"r\")\n",
    "ax2.tick_params(\"y\", colors=\"r\")\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-respect",
   "metadata": {},
   "source": [
    "### tf.keras schedulers\n",
    "TensorFlow has nice custom schedulers that are easy to use and their state is actually persisted together with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "recent-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5940 - accuracy: 0.7928 - val_loss: 0.4016 - val_accuracy: 0.8640\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3869 - accuracy: 0.8634 - val_loss: 0.3673 - val_accuracy: 0.8746\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3498 - accuracy: 0.8773 - val_loss: 0.3640 - val_accuracy: 0.8726\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3265 - accuracy: 0.8842 - val_loss: 0.3443 - val_accuracy: 0.8782\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3141 - accuracy: 0.8864 - val_loss: 0.3380 - val_accuracy: 0.8778\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2881 - accuracy: 0.8957 - val_loss: 0.3368 - val_accuracy: 0.8796\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2835 - accuracy: 0.8989 - val_loss: 0.3298 - val_accuracy: 0.8848\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2684 - accuracy: 0.9053 - val_loss: 0.3334 - val_accuracy: 0.8802\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2688 - accuracy: 0.9034 - val_loss: 0.3228 - val_accuracy: 0.8866\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2546 - accuracy: 0.9095 - val_loss: 0.3209 - val_accuracy: 0.8864\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2504 - accuracy: 0.9110 - val_loss: 0.3214 - val_accuracy: 0.8880\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2438 - accuracy: 0.9146 - val_loss: 0.3264 - val_accuracy: 0.8854\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2397 - accuracy: 0.9166 - val_loss: 0.3175 - val_accuracy: 0.8898\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2367 - accuracy: 0.9161 - val_loss: 0.3195 - val_accuracy: 0.8914\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2364 - accuracy: 0.9166 - val_loss: 0.3170 - val_accuracy: 0.8878\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2313 - accuracy: 0.9199 - val_loss: 0.3154 - val_accuracy: 0.8902\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2259 - accuracy: 0.9214 - val_loss: 0.3170 - val_accuracy: 0.8896\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2270 - accuracy: 0.9205 - val_loss: 0.3149 - val_accuracy: 0.8900\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2285 - accuracy: 0.9215 - val_loss: 0.3170 - val_accuracy: 0.8894\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2279 - accuracy: 0.9212 - val_loss: 0.3141 - val_accuracy: 0.8894\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2263 - accuracy: 0.9218 - val_loss: 0.3150 - val_accuracy: 0.8898\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2241 - accuracy: 0.9214 - val_loss: 0.3145 - val_accuracy: 0.8908\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2211 - accuracy: 0.9236 - val_loss: 0.3145 - val_accuracy: 0.8888\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2182 - accuracy: 0.9242 - val_loss: 0.3143 - val_accuracy: 0.8890\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2215 - accuracy: 0.9235 - val_loss: 0.3145 - val_accuracy: 0.8914\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# Crate an exponential decay TF scheduler and pass it directly to the SGD optimizer\n",
    "s = 20 * len(X_train) // batch_size # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Build a model using the optimizer\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 25 epochs\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-membrane",
   "metadata": {},
   "source": [
    "For the same piecewise constant scheduler we defined earlier one can use:\n",
    "```python\n",
    "learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],\n",
    "    values=[0.01, 0.005, 0.001],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-investor",
   "metadata": {},
   "source": [
    "### 1Cycle scheduling\n",
    "*1Cycle scheduling* is a simple schedule that surprisingly works quite well in practice. The idea is to use three learing rates which are incremented/decremented linearly:\n",
    "1. *initial learning rate* is linearly incremented from the start of the learning\n",
    "1. *maximum learning rate* is the the turning point after which the learning rate is linearly decreased\n",
    "1. the last few steps we use the *final learning rate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "literary-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.6605 - accuracy: 0.7701 - val_loss: 0.4169 - val_accuracy: 0.8578\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4042 - accuracy: 0.8561 - val_loss: 0.3785 - val_accuracy: 0.8686\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3644 - accuracy: 0.8719 - val_loss: 0.4103 - val_accuracy: 0.8550\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3407 - accuracy: 0.8766 - val_loss: 0.3502 - val_accuracy: 0.8760\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3252 - accuracy: 0.8823 - val_loss: 0.3458 - val_accuracy: 0.8764\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2893 - accuracy: 0.8946 - val_loss: 0.3509 - val_accuracy: 0.8776\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2807 - accuracy: 0.8971 - val_loss: 0.3862 - val_accuracy: 0.8656\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2651 - accuracy: 0.9025 - val_loss: 0.3763 - val_accuracy: 0.8698\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2570 - accuracy: 0.9038 - val_loss: 0.3317 - val_accuracy: 0.8830\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2417 - accuracy: 0.9100 - val_loss: 0.3533 - val_accuracy: 0.8812\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2267 - accuracy: 0.9159 - val_loss: 0.3378 - val_accuracy: 0.8872\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.2180 - accuracy: 0.9191 - val_loss: 0.3802 - val_accuracy: 0.8732\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2001 - accuracy: 0.9257 - val_loss: 0.3572 - val_accuracy: 0.8866\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.1816 - accuracy: 0.9324 - val_loss: 0.3786 - val_accuracy: 0.8754\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1639 - accuracy: 0.9402 - val_loss: 0.3543 - val_accuracy: 0.8894\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.1464 - accuracy: 0.9462 - val_loss: 0.3454 - val_accuracy: 0.8890\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.1247 - accuracy: 0.9556 - val_loss: 0.3655 - val_accuracy: 0.8900\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1159 - accuracy: 0.9584 - val_loss: 0.3621 - val_accuracy: 0.8928\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.1040 - accuracy: 0.9628 - val_loss: 0.3572 - val_accuracy: 0.8942\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0928 - accuracy: 0.9680 - val_loss: 0.3622 - val_accuracy: 0.8954\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0823 - accuracy: 0.9733 - val_loss: 0.3696 - val_accuracy: 0.8970\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0735 - accuracy: 0.9762 - val_loss: 0.3678 - val_accuracy: 0.8952\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0678 - accuracy: 0.9795 - val_loss: 0.3677 - val_accuracy: 0.9000\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 0.0624 - accuracy: 0.9822 - val_loss: 0.3705 - val_accuracy: 0.8984\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.0597 - accuracy: 0.9835 - val_loss: 0.3697 - val_accuracy: 0.8956\n"
     ]
    }
   ],
   "source": [
    "# Reset the RNG state\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a 1Cycle scheduler callback\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None, last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    \n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "            \n",
    "        # Update current learning rate and iteration\n",
    "        self.iteration += 1\n",
    "        keras.backend.set_value(self.model.optimizer.lr, rate)\n",
    "\n",
    "# Build the same model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.SGD(lr=1e-3), metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 25 epochs using the 1Cycle learning rate schedule\n",
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(iterations=len(X_train) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid), callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-beauty",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-payday",
   "metadata": {},
   "source": [
    "### $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "understood-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 18s 9ms/step - loss: 3.2680 - accuracy: 0.7932 - val_loss: 0.7201 - val_accuracy: 0.8306\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.7299 - accuracy: 0.8239 - val_loss: 0.6843 - val_accuracy: 0.8368\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "# l2 regularization with factor 0.01 (l1 works analogously)\n",
    "#  - additionally there is `l1_l2(0.1, 0.01)` regularizator wihich takes two parameters\n",
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "# Template for a regularized dense layer\n",
    "RegularizedDense = partial(\n",
    "    keras.layers.Dense,\n",
    "    activation=\"elu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    kernel_regularizer=keras.regularizers.l2(0.01),\n",
    ")\n",
    "\n",
    "# Build a l2 regularized model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 2 epochs\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-compatibility",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout is simple yet successful regularization technique which works as follows: During training we for each instance randomly with probability $p$ switch off neurons (i.e. output forced to be 0) in all but the output layer (i.e. including inputs). This way each neuron has to learn something useful and cannot be dependent on other neurons which makes the network as a whole more robust.\n",
    "\n",
    "A small adjustment has to be made: because training phase randomly switches neurons off, simply applying learned weights to test instances would increase the input signal. To compensate for this effect, after training all weights from dropout layers are reduced accordingly.\n",
    "\n",
    "Finally, dropout regularization prolongs the convergence (i.e. longer training) but the model typically generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "later-threat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 17s 9ms/step - loss: 0.7128 - accuracy: 0.7649 - val_loss: 0.3672 - val_accuracy: 0.8656\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4319 - accuracy: 0.8423 - val_loss: 0.3477 - val_accuracy: 0.8674\n"
     ]
    }
   ],
   "source": [
    "# Dropout can be implemented by adding extra `Dropout` layers\n",
    "#  - these layers automatically adjust weights after training\n",
    "#  - however, it is good to re-evaluate training and validation erros after training\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for few epochs\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-interface",
   "metadata": {},
   "source": [
    "### Alpha Dropout\n",
    "*Alpha Dropout* is a variation of droptout regularization that preserves the mean and variance of neuron outputs for self-normalizing networks (e.g. using *SELU* activations) where standard dropout would break this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "prepared-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 10s 5ms/step - loss: 0.8023 - accuracy: 0.7146 - val_loss: 0.5778 - val_accuracy: 0.8446\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.5662 - accuracy: 0.7903 - val_loss: 0.5161 - val_accuracy: 0.8518\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5260 - accuracy: 0.8061 - val_loss: 0.4906 - val_accuracy: 0.8596\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5126 - accuracy: 0.8094 - val_loss: 0.4828 - val_accuracy: 0.8602\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.5076 - accuracy: 0.8129 - val_loss: 0.4254 - val_accuracy: 0.8692\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4795 - accuracy: 0.8206 - val_loss: 0.4681 - val_accuracy: 0.8642\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4717 - accuracy: 0.8265 - val_loss: 0.4701 - val_accuracy: 0.8618\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4570 - accuracy: 0.8288 - val_loss: 0.4201 - val_accuracy: 0.8678\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4623 - accuracy: 0.8287 - val_loss: 0.4282 - val_accuracy: 0.8756\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4547 - accuracy: 0.8319 - val_loss: 0.4271 - val_accuracy: 0.8644\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4461 - accuracy: 0.8339 - val_loss: 0.4218 - val_accuracy: 0.8734\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4410 - accuracy: 0.8357 - val_loss: 0.5224 - val_accuracy: 0.8548\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4347 - accuracy: 0.8381 - val_loss: 0.4341 - val_accuracy: 0.8738\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4314 - accuracy: 0.8400 - val_loss: 0.4583 - val_accuracy: 0.8636\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4322 - accuracy: 0.8376 - val_loss: 0.4392 - val_accuracy: 0.8654\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4264 - accuracy: 0.8397 - val_loss: 0.4253 - val_accuracy: 0.8740\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4212 - accuracy: 0.8421 - val_loss: 0.5383 - val_accuracy: 0.8556\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4369 - accuracy: 0.8377 - val_loss: 0.4623 - val_accuracy: 0.8724\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4262 - accuracy: 0.8398 - val_loss: 0.4693 - val_accuracy: 0.8710\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.4186 - accuracy: 0.8419 - val_loss: 0.4436 - val_accuracy: 0.8716\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4809 - accuracy: 0.8584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48092955350875854, 0.8583999872207642]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the RNG state\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build a model using dense layers with SELU activations\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-continent",
   "metadata": {},
   "source": [
    "### MC Dropout\n",
    "*Monte-Carlo Droupout* is a useful addition to standard dropout that improves model's prediction confidence and slightly accuracy. It works as follows:\n",
    "1. Train a model with droupout as usual\n",
    "1. Instead of single prediction make several with training flag turned on (to force new dropout each time)\n",
    "1. Return a mean prediction over these samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "involved-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the RNG state\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "# Use the last model to make 100 predictions (each with new dropout) and stack them\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) for _ in range(n_samples)])\n",
    "\n",
    "# Final prediction is the mean over these samples\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fossil-cornwall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "massive-network",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.51, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.76, 0.  , 0.23]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.  , 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.36, 0.  , 0.61]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.27, 0.  , 0.73]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.6 , 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.32, 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.1 , 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.08, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.21, 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.28, 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.31, 0.  , 0.5 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.41, 0.  , 0.58]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.07, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.6 , 0.  , 0.36]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.81, 0.  , 0.19]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.07, 0.  , 0.85]],\n",
       "\n",
       "       [[0.02, 0.  , 0.  , 0.  , 0.  , 0.18, 0.11, 0.06, 0.  , 0.62]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.09, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.47, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.15, 0.  , 0.81]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.19, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.13, 0.  , 0.62]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.55, 0.  , 0.27]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.  , 0.  , 0.67]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.54, 0.  , 0.14]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.09, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.69, 0.  , 0.26]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.22, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.01, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.2 , 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.45, 0.  , 0.24]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.08, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.1 , 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.08, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.08, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.01, 0.  , 0.03, 0.06, 0.01, 0.56, 0.  , 0.34]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.02, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.27, 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.19, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.23, 0.  , 0.18, 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.03, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.39, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.63, 0.  , 0.24]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.02, 0.34, 0.  , 0.23, 0.  , 0.4 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.07, 0.  , 0.62]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.44, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.15, 0.  , 0.76]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.26, 0.  , 0.72]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.07, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.65, 0.  , 0.34]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.38, 0.  , 0.57]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.03, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.17, 0.  , 0.83]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.33, 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.87, 0.  , 0.06, 0.  , 0.07]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.02, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.13, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.15, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.52, 0.  , 0.32]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.56, 0.  , 0.28]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.35, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.17, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.13, 0.  , 0.77]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "lasting-exception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.19, 0.  , 0.73]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "going-vermont",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.01, 0.2 , 0.  , 0.24]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "arabic-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8644"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "pleased-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_17 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout (MCAlphaDro (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_258 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_1 (MCAlphaD (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_259 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_2 (MCAlphaD (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reset the RNG state\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# For networks with special layers (e.g. batch normalized) one can subclass the `Dropout` class\n",
    "\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "# Reuse previous model but replace all Dropout layers with custom MC Dropout\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "# Show the model structure\n",
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "utility-disabled",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.21, 0.  , 0.69]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the MC model\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Pass weights from previous (non-MC) model to the MC model\n",
    "mc_model.set_weights(model.get_weights())\n",
    "\n",
    "def mc_predict(X):\n",
    "    return np.mean([mc_model.predict(X) for _ in range(n_samples)], axis=0)\n",
    "\n",
    "# Make a MC prediction (and round it)\n",
    "np.round(mc_predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-ranch",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization\n",
    "Last but not least techinque is *max-norm regularization*. This approach constrains each neuron's weights $\\mathbf{w}$ by as new hyper-parameter $r$ as follows: $\\|\\mathbf{w}\\|_2 \\le r$.\n",
    "\n",
    "However, this constraint is not added to the loss function but after each training step the weights are rescaled if needed: $\\mathbf{w} \\gets \\mathbf{w} \\frac{r}{\\|\\mathbf{2}\\|_2}$.\n",
    "\n",
    "Finall this regularization can both prevent overfitting as well as help with the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ranking-rubber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 17s 9ms/step - loss: 0.5754 - accuracy: 0.8028 - val_loss: 0.3723 - val_accuracy: 0.8654\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.3572 - accuracy: 0.8689 - val_loss: 0.3734 - val_accuracy: 0.8670\n"
     ]
    }
   ],
   "source": [
    "# Create new template for max-norm dense layers\n",
    "#  - `kernel_constraint` function is applied to layers weights\n",
    "#  - `kernel_constraint` can also be used to rescale biases\n",
    "MaxNormDense = partial(\n",
    "    keras.layers.Dense,\n",
    "    activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "    kernel_constraint=keras.constraints.max_norm(1.),\n",
    ")\n",
    "\n",
    "# Build a model with max-norm dense layers\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Trian the model for 2 epochs\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-recommendation",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-victoria",
   "metadata": {},
   "source": [
    "### Deep Learning on CIFAR10\n",
    "Build a DNN with 20 hidden layers of 100 neurons each. Use He initialization and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "wireless-being",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 27s 15ms/step - loss: 9.5594 - accuracy: 0.1386 - val_loss: 2.1826 - val_accuracy: 0.2166\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 2.0939 - accuracy: 0.2397 - val_loss: 2.1387 - val_accuracy: 0.2210\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.9633 - accuracy: 0.2788 - val_loss: 1.9865 - val_accuracy: 0.2802\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.8826 - accuracy: 0.3152 - val_loss: 2.0808 - val_accuracy: 0.2704\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.8108 - accuracy: 0.3417 - val_loss: 1.7929 - val_accuracy: 0.3438\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7575 - accuracy: 0.3618 - val_loss: 1.7592 - val_accuracy: 0.3676\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.7076 - accuracy: 0.3841 - val_loss: 1.6908 - val_accuracy: 0.3866\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.6711 - accuracy: 0.4004 - val_loss: 1.6556 - val_accuracy: 0.4096\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6461 - accuracy: 0.4111 - val_loss: 1.6766 - val_accuracy: 0.3966\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6174 - accuracy: 0.4212 - val_loss: 1.6514 - val_accuracy: 0.3946\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5861 - accuracy: 0.4309 - val_loss: 1.6397 - val_accuracy: 0.4226\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.5748 - accuracy: 0.4347 - val_loss: 1.6170 - val_accuracy: 0.4272\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5542 - accuracy: 0.4438 - val_loss: 1.6441 - val_accuracy: 0.4154\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.5310 - accuracy: 0.4484 - val_loss: 1.5805 - val_accuracy: 0.4398\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.5267 - accuracy: 0.4518 - val_loss: 1.6166 - val_accuracy: 0.4192\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5118 - accuracy: 0.4623 - val_loss: 1.5615 - val_accuracy: 0.4384\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4910 - accuracy: 0.4625 - val_loss: 1.5548 - val_accuracy: 0.4502\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4755 - accuracy: 0.4733 - val_loss: 1.5830 - val_accuracy: 0.4400\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.4576 - accuracy: 0.4788 - val_loss: 1.5542 - val_accuracy: 0.4460\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.4481 - accuracy: 0.4798 - val_loss: 1.5557 - val_accuracy: 0.4504\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4384 - accuracy: 0.4844 - val_loss: 1.5291 - val_accuracy: 0.4578\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4290 - accuracy: 0.4868 - val_loss: 1.5460 - val_accuracy: 0.4474\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4082 - accuracy: 0.4929 - val_loss: 1.5738 - val_accuracy: 0.4278\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3937 - accuracy: 0.4982 - val_loss: 1.5679 - val_accuracy: 0.4416\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3858 - accuracy: 0.5032 - val_loss: 1.5122 - val_accuracy: 0.4570\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3663 - accuracy: 0.5067 - val_loss: 1.5331 - val_accuracy: 0.4544\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3633 - accuracy: 0.5152 - val_loss: 1.6069 - val_accuracy: 0.4370\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3521 - accuracy: 0.5176 - val_loss: 1.5320 - val_accuracy: 0.4504\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3490 - accuracy: 0.5154 - val_loss: 1.5129 - val_accuracy: 0.4664\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3413 - accuracy: 0.5242 - val_loss: 1.5682 - val_accuracy: 0.4522\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3388 - accuracy: 0.5176 - val_loss: 1.5579 - val_accuracy: 0.4548\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3110 - accuracy: 0.5273 - val_loss: 1.4965 - val_accuracy: 0.4710\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3142 - accuracy: 0.5280 - val_loss: 1.5344 - val_accuracy: 0.4630\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3012 - accuracy: 0.5329 - val_loss: 1.5061 - val_accuracy: 0.4752\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2958 - accuracy: 0.5365 - val_loss: 1.5377 - val_accuracy: 0.4680\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.2852 - accuracy: 0.5426 - val_loss: 1.5283 - val_accuracy: 0.4704\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.2747 - accuracy: 0.5434 - val_loss: 1.5110 - val_accuracy: 0.4780\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2599 - accuracy: 0.5502 - val_loss: 1.5079 - val_accuracy: 0.4674\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2616 - accuracy: 0.5464 - val_loss: 1.5210 - val_accuracy: 0.4714\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2540 - accuracy: 0.5503 - val_loss: 1.5775 - val_accuracy: 0.4638\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.2373 - accuracy: 0.5568 - val_loss: 1.5165 - val_accuracy: 0.4756\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.2331 - accuracy: 0.5621 - val_loss: 1.5359 - val_accuracy: 0.4782\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2373 - accuracy: 0.5565 - val_loss: 1.5225 - val_accuracy: 0.4682\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2175 - accuracy: 0.5630 - val_loss: 1.5987 - val_accuracy: 0.4514\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2117 - accuracy: 0.5628 - val_loss: 1.5196 - val_accuracy: 0.4768\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1893 - accuracy: 0.5758 - val_loss: 1.5372 - val_accuracy: 0.4748\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1936 - accuracy: 0.5735 - val_loss: 1.5664 - val_accuracy: 0.4632\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1893 - accuracy: 0.5722 - val_loss: 1.5362 - val_accuracy: 0.4744\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1902 - accuracy: 0.5706 - val_loss: 1.5817 - val_accuracy: 0.4716\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.1814 - accuracy: 0.5778 - val_loss: 1.5741 - val_accuracy: 0.4656\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 1.1708 - accuracy: 0.5830 - val_loss: 1.5499 - val_accuracy: 0.4758\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1607 - accuracy: 0.5847 - val_loss: 1.5730 - val_accuracy: 0.4686\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4965 - accuracy: 0.4710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4964699745178223, 0.47099998593330383]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset TF session and RNG state\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load and split the CIFAR10 dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "val_split = 5000\n",
    "X_train = X_train_full[val_split:]\n",
    "y_train = y_train_full[val_split:]\n",
    "X_valid = X_train_full[:val_split]\n",
    "y_valid = y_train_full[:val_split]\n",
    "\n",
    "# Build the model\n",
    "n_hidden = 20\n",
    "\n",
    "inputs = [keras.layers.Flatten(input_shape=[32, 32, 3])]\n",
    "hidden = [\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "    for _ in range(n_hidden)\n",
    "]\n",
    "outputs = [keras.layers.Dense(10, activation=\"softmax\")]\n",
    "\n",
    "model = keras.models.Sequential(inputs + hidden + outputs)\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Make necessary callbacks\n",
    "model_path = os.path.join(\"data\", \"my_cifar10_model.h5\")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(model_path, save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(\"logs\", \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "# Train the model for 100 epochs with early stopping\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)\n",
    "\n",
    "# Load and evaluate the best model\n",
    "model = keras.models.load_model(model_path)\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-national",
   "metadata": {},
   "source": [
    "Now let's try adding Batch Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "amazing-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 53s 25ms/step - loss: 1.9801 - accuracy: 0.2895 - val_loss: 1.6528 - val_accuracy: 0.4022\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.6768 - accuracy: 0.4019 - val_loss: 1.5797 - val_accuracy: 0.4302\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.6136 - accuracy: 0.4290 - val_loss: 1.5193 - val_accuracy: 0.4554\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.5459 - accuracy: 0.4490 - val_loss: 1.4863 - val_accuracy: 0.4716\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.5037 - accuracy: 0.4664 - val_loss: 1.4484 - val_accuracy: 0.4826\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.4653 - accuracy: 0.4770 - val_loss: 1.4327 - val_accuracy: 0.4908\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.4323 - accuracy: 0.4906 - val_loss: 1.4157 - val_accuracy: 0.4966\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.4063 - accuracy: 0.5036 - val_loss: 1.3731 - val_accuracy: 0.5136\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3780 - accuracy: 0.5086 - val_loss: 1.3785 - val_accuracy: 0.5076\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.3538 - accuracy: 0.5194 - val_loss: 1.3677 - val_accuracy: 0.5090\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3304 - accuracy: 0.5325 - val_loss: 1.3414 - val_accuracy: 0.5264\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3079 - accuracy: 0.5379 - val_loss: 1.3802 - val_accuracy: 0.5048\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2925 - accuracy: 0.5436 - val_loss: 1.3731 - val_accuracy: 0.5140\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2655 - accuracy: 0.5513 - val_loss: 1.3430 - val_accuracy: 0.5316\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2608 - accuracy: 0.5575 - val_loss: 1.3898 - val_accuracy: 0.5122\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2444 - accuracy: 0.5590 - val_loss: 1.3464 - val_accuracy: 0.5326\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.2311 - accuracy: 0.5605 - val_loss: 1.3119 - val_accuracy: 0.5352\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.2131 - accuracy: 0.5707 - val_loss: 1.3372 - val_accuracy: 0.5368\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1953 - accuracy: 0.5780 - val_loss: 1.3596 - val_accuracy: 0.5240\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1913 - accuracy: 0.5789 - val_loss: 1.3586 - val_accuracy: 0.5272\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1728 - accuracy: 0.5847 - val_loss: 1.3461 - val_accuracy: 0.5348\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1617 - accuracy: 0.5901 - val_loss: 1.3478 - val_accuracy: 0.5322\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.1474 - accuracy: 0.5929 - val_loss: 1.3296 - val_accuracy: 0.5430\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1356 - accuracy: 0.5977 - val_loss: 1.3294 - val_accuracy: 0.5434\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1218 - accuracy: 0.6052 - val_loss: 1.3351 - val_accuracy: 0.5410\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.1051 - accuracy: 0.6058 - val_loss: 1.3251 - val_accuracy: 0.5420\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0946 - accuracy: 0.6167 - val_loss: 1.3318 - val_accuracy: 0.5426\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0903 - accuracy: 0.6150 - val_loss: 1.3640 - val_accuracy: 0.5224\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0843 - accuracy: 0.6139 - val_loss: 1.3092 - val_accuracy: 0.5490\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0664 - accuracy: 0.6262 - val_loss: 1.3457 - val_accuracy: 0.5332\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.0591 - accuracy: 0.6230 - val_loss: 1.3439 - val_accuracy: 0.5420\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0354 - accuracy: 0.6338 - val_loss: 1.3451 - val_accuracy: 0.5440\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.0349 - accuracy: 0.6360 - val_loss: 1.3480 - val_accuracy: 0.5454\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0261 - accuracy: 0.6370 - val_loss: 1.3469 - val_accuracy: 0.5410\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 33s 23ms/step - loss: 1.0183 - accuracy: 0.6437 - val_loss: 1.3537 - val_accuracy: 0.5384\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0056 - accuracy: 0.6428 - val_loss: 1.3502 - val_accuracy: 0.5444\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9875 - accuracy: 0.6530 - val_loss: 1.3554 - val_accuracy: 0.5454\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9815 - accuracy: 0.6507 - val_loss: 1.3536 - val_accuracy: 0.5420\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9710 - accuracy: 0.6538 - val_loss: 1.3882 - val_accuracy: 0.5320\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9693 - accuracy: 0.6539 - val_loss: 1.3844 - val_accuracy: 0.5444\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9529 - accuracy: 0.6608 - val_loss: 1.3655 - val_accuracy: 0.5368\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9531 - accuracy: 0.6652 - val_loss: 1.3735 - val_accuracy: 0.5486\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9532 - accuracy: 0.6668 - val_loss: 1.3792 - val_accuracy: 0.5368\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 0.9441 - accuracy: 0.6700 - val_loss: 1.3937 - val_accuracy: 0.5424\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9229 - accuracy: 0.6748 - val_loss: 1.3818 - val_accuracy: 0.5374\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 0.9089 - accuracy: 0.6818 - val_loss: 1.4038 - val_accuracy: 0.5354\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 0.9070 - accuracy: 0.6808 - val_loss: 1.3983 - val_accuracy: 0.5394\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 0.9085 - accuracy: 0.6791 - val_loss: 1.4049 - val_accuracy: 0.5440\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 0.9006 - accuracy: 0.6844 - val_loss: 1.4240 - val_accuracy: 0.5384\n",
      "157/157 [==============================] - 2s 5ms/step - loss: 1.3092 - accuracy: 0.5490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3092141151428223, 0.5490000247955322]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Reset TF session and RNG state\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build the model\n",
    "n_hidden = 20\n",
    "\n",
    "def build_hidden_layer():\n",
    "    dense = keras.layers.Dense(100, kernel_initializer=\"he_normal\")\n",
    "    bn = keras.layers.BatchNormalization()\n",
    "    activation = keras.layers.Activation(\"elu\")\n",
    "    return dense, bn, activation\n",
    "\n",
    "inputs = [keras.layers.Flatten(input_shape=[32, 32, 3]), keras.layers.BatchNormalization()]\n",
    "hidden = list(chain.from_iterable(build_hidden_layer() for _ in range(n_hidden)))\n",
    "outputs = [keras.layers.Dense(10, activation=\"softmax\")]\n",
    "\n",
    "model = keras.models.Sequential(inputs + hidden + outputs)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Nadam(lr=5e-4),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Make necessary callbacks\n",
    "model_path = os.path.join(\"data\", \"my_cifar10_bn_model.h5\")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(model_path, save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(\"logs\", \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "# Train the model for 100 epochs with early stopping\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks)\n",
    "\n",
    "# Load and evaluate the best model\n",
    "model = keras.models.load_model(model_path)\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-cincinnati",
   "metadata": {},
   "source": [
    "Let's try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "simple-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 29s 16ms/step - loss: 2.0569 - accuracy: 0.2692 - val_loss: 1.9222 - val_accuracy: 0.2946\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.7364 - accuracy: 0.3772 - val_loss: 1.7236 - val_accuracy: 0.3810\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.6291 - accuracy: 0.4230 - val_loss: 1.6511 - val_accuracy: 0.4114\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5421 - accuracy: 0.4552 - val_loss: 1.6436 - val_accuracy: 0.4262\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4883 - accuracy: 0.4738 - val_loss: 1.5848 - val_accuracy: 0.4354\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4310 - accuracy: 0.5004 - val_loss: 1.5300 - val_accuracy: 0.4614\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3827 - accuracy: 0.5172 - val_loss: 1.5179 - val_accuracy: 0.4648\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3390 - accuracy: 0.5298 - val_loss: 1.4853 - val_accuracy: 0.4806\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3108 - accuracy: 0.5443 - val_loss: 1.4671 - val_accuracy: 0.4698\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2754 - accuracy: 0.5575 - val_loss: 1.5031 - val_accuracy: 0.4882\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2376 - accuracy: 0.5714 - val_loss: 1.5341 - val_accuracy: 0.4764\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2163 - accuracy: 0.5784 - val_loss: 1.5308 - val_accuracy: 0.4822\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 1.1861 - accuracy: 0.5886 - val_loss: 1.4841 - val_accuracy: 0.5056\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1466 - accuracy: 0.6035 - val_loss: 1.4609 - val_accuracy: 0.5152\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1360 - accuracy: 0.6110 - val_loss: 1.5645 - val_accuracy: 0.4946\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1067 - accuracy: 0.6173 - val_loss: 1.5007 - val_accuracy: 0.5130\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0870 - accuracy: 0.6271 - val_loss: 1.5073 - val_accuracy: 0.5072\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0718 - accuracy: 0.6335 - val_loss: 1.5059 - val_accuracy: 0.4844\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0490 - accuracy: 0.6401 - val_loss: 1.5206 - val_accuracy: 0.5184\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0161 - accuracy: 0.6520 - val_loss: 1.5802 - val_accuracy: 0.4940\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0121 - accuracy: 0.6504 - val_loss: 1.5646 - val_accuracy: 0.5046\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9880 - accuracy: 0.6619 - val_loss: 1.5574 - val_accuracy: 0.4962\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9920 - accuracy: 0.6598 - val_loss: 1.5521 - val_accuracy: 0.4990\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9560 - accuracy: 0.6752 - val_loss: 1.5958 - val_accuracy: 0.5090\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9465 - accuracy: 0.6781 - val_loss: 1.5816 - val_accuracy: 0.5028\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9157 - accuracy: 0.6906 - val_loss: 1.8659 - val_accuracy: 0.4016\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2484 - accuracy: 0.5716 - val_loss: 1.5824 - val_accuracy: 0.4922\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0295 - accuracy: 0.6471 - val_loss: 1.5996 - val_accuracy: 0.4900\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9646 - accuracy: 0.6696 - val_loss: 24069.1055 - val_accuracy: 0.5116\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 58.1520 - accuracy: 0.6179 - val_loss: 1.7069 - val_accuracy: 0.4160\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3814 - accuracy: 0.5157 - val_loss: 1.6009 - val_accuracy: 0.4610\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2168 - accuracy: 0.5721 - val_loss: 1.5859 - val_accuracy: 0.4634\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1799 - accuracy: 0.5889 - val_loss: 1.5665 - val_accuracy: 0.4814\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1175 - accuracy: 0.6073 - val_loss: 2.5405 - val_accuracy: 0.4768\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 1.4609 - accuracy: 0.5152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4608896970748901, 0.5152000188827515]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset TF session and RNG state\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build the model\n",
    "n_hidden = 20\n",
    "\n",
    "inputs = [keras.layers.Flatten(input_shape=[32, 32, 3])]\n",
    "hidden = [\n",
    "    keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\")\n",
    "    for _ in range(n_hidden)\n",
    "]\n",
    "outputs = [keras.layers.Dense(10, activation=\"softmax\")]\n",
    "\n",
    "model = keras.models.Sequential(inputs + hidden + outputs)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Nadam(lr=7e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Make necessary callbacks\n",
    "model_path = os.path.join(\"data\", \"my_cifar10_selu_model.h5\")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(model_path, save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(\"logs\", \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "# Normalize the inputs\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "# Train the model for 100 epochs with early stopping\n",
    "model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks)\n",
    "\n",
    "# Load and evaluate the best model\n",
    "model = keras.models.load_model(model_path)\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-guidance",
   "metadata": {},
   "source": [
    "Let's try regularizing the model with alpha dropout and then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "functional-baltimore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 19s 11ms/step - loss: 2.2196 - accuracy: 0.3155 - val_loss: 1.7221 - val_accuracy: 0.4444\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.6319 - accuracy: 0.4400 - val_loss: 1.6066 - val_accuracy: 0.4468\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.5096 - accuracy: 0.4705 - val_loss: 1.4944 - val_accuracy: 0.4896\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.4412 - accuracy: 0.4927 - val_loss: 1.4817 - val_accuracy: 0.4892\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3936 - accuracy: 0.5122 - val_loss: 1.4751 - val_accuracy: 0.4968\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3612 - accuracy: 0.5200 - val_loss: 1.4313 - val_accuracy: 0.5048\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3343 - accuracy: 0.5270 - val_loss: 1.4548 - val_accuracy: 0.4992\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.3012 - accuracy: 0.5436 - val_loss: 1.4283 - val_accuracy: 0.5116\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2890 - accuracy: 0.5452 - val_loss: 1.4447 - val_accuracy: 0.5178\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2567 - accuracy: 0.5572 - val_loss: 1.4483 - val_accuracy: 0.5132\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2335 - accuracy: 0.5680 - val_loss: 1.4110 - val_accuracy: 0.5240\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2164 - accuracy: 0.5702 - val_loss: 1.4470 - val_accuracy: 0.5200\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.2019 - accuracy: 0.5796 - val_loss: 1.4792 - val_accuracy: 0.5096\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1788 - accuracy: 0.5875 - val_loss: 1.4382 - val_accuracy: 0.5282\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1648 - accuracy: 0.5929 - val_loss: 1.4748 - val_accuracy: 0.5234\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1607 - accuracy: 0.5929 - val_loss: 1.4774 - val_accuracy: 0.5254\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1442 - accuracy: 0.5955 - val_loss: 1.4613 - val_accuracy: 0.5308\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1300 - accuracy: 0.6046 - val_loss: 1.4780 - val_accuracy: 0.5180\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.1143 - accuracy: 0.6072 - val_loss: 1.4774 - val_accuracy: 0.5264\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0978 - accuracy: 0.6155 - val_loss: 1.4926 - val_accuracy: 0.5174\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0893 - accuracy: 0.6150 - val_loss: 1.4951 - val_accuracy: 0.5206\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0813 - accuracy: 0.6202 - val_loss: 1.4733 - val_accuracy: 0.5236\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0677 - accuracy: 0.6214 - val_loss: 1.5198 - val_accuracy: 0.5200\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0548 - accuracy: 0.6272 - val_loss: 1.5137 - val_accuracy: 0.5224\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0476 - accuracy: 0.6304 - val_loss: 1.5293 - val_accuracy: 0.5266\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0225 - accuracy: 0.6385 - val_loss: 1.5298 - val_accuracy: 0.5184\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0215 - accuracy: 0.6403 - val_loss: 1.5301 - val_accuracy: 0.5218\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0091 - accuracy: 0.6479 - val_loss: 1.5284 - val_accuracy: 0.5230\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0172 - accuracy: 0.6390 - val_loss: 1.5308 - val_accuracy: 0.5302\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 1.0006 - accuracy: 0.6470 - val_loss: 1.5404 - val_accuracy: 0.5254\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 14s 10ms/step - loss: 0.9938 - accuracy: 0.6482 - val_loss: 1.5737 - val_accuracy: 0.5298\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4110 - accuracy: 0.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4110283851623535, 0.5239999890327454]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset TF session and RNG state\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build the model\n",
    "inputs = [keras.layers.Flatten(input_shape=[32, 32, 3])]\n",
    "hidden = [keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\")] + [keras.layers.AlphaDropout(rate=0.1)]\n",
    "outputs = [keras.layers.Dense(10, activation=\"softmax\")]\n",
    "\n",
    "model = keras.models.Sequential(inputs + hidden + outputs)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Nadam(lr=5e-4),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Make necessary callbacks\n",
    "model_path = os.path.join(\"data\", \"my_cifar10_alpha_dropout_model.h5\")\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(model_path, save_best_only=True)\n",
    "\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "# Normalize the inputs\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "# Train the model for 100 epochs with early stoppings\n",
    "model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks)\n",
    "\n",
    "# Load and evaluate the best model\n",
    "model = keras.models.load_model(model_path)\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "healthy-moscow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "\n",
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)\n",
    "\n",
    "\n",
    "# Reset TF session and RNG state\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Replace dropout layer by MC dropout\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "\n",
    "# Make prediction and compute accuracy of the MC model\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-insert",
   "metadata": {},
   "source": [
    "Finally, lets' retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "phantom-richmond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 4s 10ms/step - loss: 2.1928 - accuracy: 0.2610 - val_loss: 1.8953 - val_accuracy: 0.3784\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.8472 - accuracy: 0.3669 - val_loss: 1.6621 - val_accuracy: 0.4308\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.7053 - accuracy: 0.4032 - val_loss: 1.5979 - val_accuracy: 0.4478\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.5956 - accuracy: 0.4406 - val_loss: 1.5322 - val_accuracy: 0.4662\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.5269 - accuracy: 0.4630 - val_loss: 1.5032 - val_accuracy: 0.4728\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4661 - accuracy: 0.4820 - val_loss: 1.4487 - val_accuracy: 0.4948\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4383 - accuracy: 0.4951 - val_loss: 1.4342 - val_accuracy: 0.4992\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4116 - accuracy: 0.5036 - val_loss: 1.4307 - val_accuracy: 0.5026\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4164 - accuracy: 0.5045 - val_loss: 1.4307 - val_accuracy: 0.5026\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4116 - accuracy: 0.5043 - val_loss: 1.4307 - val_accuracy: 0.5030\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4035 - accuracy: 0.5092 - val_loss: 1.4306 - val_accuracy: 0.5028\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4119 - accuracy: 0.5042 - val_loss: 1.4306 - val_accuracy: 0.5026\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4146 - accuracy: 0.5073 - val_loss: 1.4306 - val_accuracy: 0.5026\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4019 - accuracy: 0.5086 - val_loss: 1.4306 - val_accuracy: 0.5026\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 1.4089 - accuracy: 0.5068 - val_loss: 1.4306 - val_accuracy: 0.5024\n"
     ]
    }
   ],
   "source": [
    "# Reset TF session and RNG state\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Build new model\n",
    "inputs = [keras.layers.Flatten(input_shape=[32, 32, 3])]\n",
    "hidden = [keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\")] + [keras.layers.AlphaDropout(rate=0.1)]\n",
    "outputs = [keras.layers.Dense(10, activation=\"softmax\")]\n",
    "\n",
    "model = keras.models.Sequential(inputs + hidden + outputs)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.SGD(lr=1e-2),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model for 15 epochs with 1Cycle learning rate scheduling\n",
    "onecycle = OneCycleScheduler(len(X_train_scaled) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=15, batch_size=128, validation_data=(X_valid_scaled, y_valid), callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-current",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
