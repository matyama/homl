{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "**Curse of dimensionality** - with increasing dimension the data become more sparse and it is harder to sample the space (size of the training set grows exponentially with the dimension). Hence the other approach, reduction of dimensionality.\n",
    "\n",
    "In essence the idea is simple, reduce dimension of the input sample while loosing as little information as possible. Generally, there are two approaches:\n",
    "* **Projecting to lower dimension** - data are projected to lower-dimensional subspace (e.g. *PCA* projects to a hyper-plane while preserving as much variance as possible)\n",
    "* **Manifold Learning** - assumes that high-dimensional data lie on a manifold and tries to model this manifold (*manifold* in $n$ dimensions is a $d < n$ dimensional subspace that locally resembles a linear space)\n",
    "\n",
    "## PCA\n",
    "*PCA* stands for *Principal Component Analysis* and is a data projection technique that projects data to a lower dimensional sub-space while preserving principal components (PC) where the data exhibit high variance. This corresponds to minimization of the *MSE* and should preserve as much information as possible.\n",
    "\n",
    "It is important to note that *PCA* expects the data to be centered. *Scikit-Learn* does this automatically and also uses *SVD* to find the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first center the data\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# use SVD to find first 2 principal components\n",
    "n_components = 2\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "W = Vt.T[:, :n_components]\n",
    "\n",
    "# project data to d dimensions\n",
    "X2D_svd = X_centered.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the right dimension to reduce to is usually (except for data visualizations) done by setting the target variance ratio to preserve. One option is to fit the data and then find the right dimension from `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# just to get the same rng state as in the book\n",
    "np.random.seed(3)\n",
    "_ = np.random.randn(200, 2) / 10\n",
    "\n",
    "# load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = 0.95\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "n_components = np.argmax(cumsum >= target_var) + 1\n",
    "\n",
    "pca = PCA(n_components)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, *Scikit-Learn*'s `PCA` accepts this target variance in `n_components` (as a `float` between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=target_var)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for compression\n",
    "\n",
    "PCA is a loss compression (some information is lost) but can greatly reduce the size of a dataset. To *decompress* the data one can apply the inverse transofrmation $\\mathbf{X}_{\\text{recovered}} = \\mathbf{X}_{d\\text{-proj}} \\mathbf{W}_d^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized PCA\n",
    "\n",
    "Runs a stochastic approximation algorithm that reduces the runtime of PCA from $\\mathcal{O}(m \\times n^2) + \\mathcal{O}(n^3)$ to $\\mathcal{O}(m \\times d^2) + \\mathcal{O}(d^3)$. By default *Scikit-Learn* uses `svd_solver='auto'` which automatically determines which algorithm to use based on the training set size, target dimension or target preserved variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver='randomized')\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental PCA\n",
    "Incremental PCA (IPCA) is an online method to split a dataset that does not fit into memory into mini-batches (or when examples arrive later) and performs PCA on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "ipca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    ipca.partial_fit(X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can use a memory-mapped data file and pass a `batch_size` to `IncrementalPCA` which then loads the data into memory as it need them (use `fit` in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'datasets/my_mnist.data'\n",
    "m, n = X_train.shape\n",
    "\n",
    "X_mm = np.memmap(filename, dtype='float32', mode='write', shape=(m, n))\n",
    "X_mm[:] = X_train\n",
    "del X_mm  # trigger its Python finalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncrementalPCA(batch_size=525, n_components=154)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "y = t > 6.9\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Kernel and Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('kpca', KernelPCA(n_components=2)),\n",
    "    ('log_reg', LogisticRegression()),\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {'kpca__gamma': np.linspace(0.03, 0.05, 10), 'kpca__kernel': ['rbf', 'sigmoid']},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method for setting hyperparameters is completely unsupervised. It's basically measuring a reconstruction error but it has a twist.\n",
    "\n",
    "Kernel trick is performing an implicit feature mapping into an infinite-dimensional feature space and therefore it's not possible to compute a reconstructed point. Fortunately it's possible to find a point in the original space that would map close to the reconstructed point (called reconstruction *pre-image*) and compute a *reconstruction pre-image error*.\n",
    "\n",
    "This can be done by fitting a regression model during training with the projected instances as the training set and original instances as targets.\n",
    "\n",
    "Finally, standard grid search with cross-validation can be used to find best hyperparameters of this *kPCA*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4299668281989978e-26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.0433, fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
